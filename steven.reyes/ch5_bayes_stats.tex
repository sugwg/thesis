\section{Probability}
Here we consider a few rules of probability theory before we begin discussing Bayesian inference and hypothesis testing. We follow the guide of~\cite{wall2012practical} for the rules of probability. Firstly, for a given set of $N$ possible outcomes where each outcome has a probability $p_i$ of occurring then the sum of all possible outcomes must be unity. This can be expressed as
\begin{equation}
 \sum_{\limit i=1}^{\limit N} p_i = 1.
\end{equation}
This is also true for probability distribution functions $p(x)$ described by a continuous parameter $x$. We express this rule of probability as
\begin{equation}
 \int p(x) \, dx = 1.
\end{equation}

Next, we consider the probability of independent events occurring and the concept of conditional probability. Two events $A$ and $B$ are said to be independent of one another if the probability of $A$ is unaffected by what we may know about $B$. This can be stated as:
\begin{equation}\label{eq:prob_ind}
 p(A \, \mathrm{and} \, B) \equiv p(A, \, B) = p(A) \, p(B).
\end{equation}
In cases that independence doesn't hold we can consider the conditional probability of $A$ given the information that we know of $B$. This conditional probability is stated as
\begin{equation}
   p(A \, | \, B) = \frac{p(A, \, B)}{p(B)}.
\end{equation}
Here $p(A \, | \, B)$ is the probability of $A$ given that $B$ has occurred. If $A$ and $B$ are independent events this reduces back to Eq.~\ref{eq:prob_ind}. If there are many possibilities for event $B$, which we label as $B_i$, then we can also attain the probability of $A$ through the following summation
\begin{equation}\label{eq:prob_marginalization}
   p(A) = \sum_{\limit i} p(A \, | \, B_i) \, p(B_i).
\end{equation}
This is called marginalization and pertains to summing out nuisance parameters. This technique of marginalization also generalizes to continuous probability distributions and can be expressed as
\begin{equation}
   p(A) = \int p(A \, | \, B) \, p(B) \, dB.
\end{equation}
In order to conduct Bayesian inference and hypothesis testing we will make extensive use of conditional probabilities and probability marginalization.

\section{Bayesian Inference}
Within statistical inference we are interested in leveraging our uncertainty regarding hypotheses and parameters and applying them to measurments or data in the real world. We seek a way to update our prior beliefs regarding these hypotheses and parameters in a way that is consistent and coherent with the data. We begin this approach by introducting Bayes' theorem:
\begin{equation} \label{eqn:BayesTheorem_basic}
     p(\mathrm{H}) \, p(\mathbf{d} \, |\, \mathrm{H})  =  p(\mathrm{H} \, | \, \mathbf{d}) p(\mathbf{d}).
\end{equation}
Here we have $p(\mathrm{H})$ which represents our prior belief about the hypothesis H. The expression $p(\mathbf{d} \, |\, \mathrm{H})$ represents the measurement of our data which is the likelihood. We use these to update our inference on the probability of hypothesis H as expressed in the posterior probability of the hypothesis given the data, $p(\mathrm{H} \, | \, \mathbf{d})$. Finally then we have $p(\mathbf{d}$, the probability of obtaining this data set. For the moment we will consider this as a normalization factor. For ease of reading we will change notation so that the prior $p(H)$ is $\pi (H)$, the likelihood $p(\mathbf{d} \, |\, \mathrm{H})$ is $\mathcal{L}(\mathbf{d} \, | \, \mathrm{H})$, the posterior distribution $p(\mathrm{H} \, | \, \mathbf{d})$ is $\mathcal{P}(\mathrm{H} \, | \, \mathbf{d})$, and the normalization factor $p(\mathbf{d})$ is $\mathcal{Z}(\mathbf{d})$ following the notation of~\cite{hobson2010bayesian}.

Within the context of parameter estimation we often use a hypothesis H that is composed from prior distributions on parameter values $\vec{\theta}$ that fully describe the hypothesis. In this context it is more helpful to rewrite Eq.~\ref{eqn:BayesTheorem_basic} as
\begin{equation}\label{eqn:BayesTheorem_PE}
   \mathcal{P}\left(\vec{\theta} \, | \, \mathbf{d}, \mathrm{H}\right) = \frac{\pi\left(\vec{\theta} \, | \, \mathrm{H}\right) \, \mathcal{L}\left(\mathbf{d} \, | \vec{\theta}\right)}{\mathcal{Z}\left(\mathbf{d} \, | \, \mathrm{H}\right)}.
\end{equation} 
In the context of Eq.~\ref{eqn:BayesTheorem_PE} the normalization constant is often called the marginal likelihood since it marginalizes all parameters of the model H out of the likelihood~\cite{hobson2010bayesian}:
\begin{equation}\label{eqn:marg_likelihood}
    \mathcal{Z}(\mathbf{d} \, | \, \mathrm{H} = \int \pi(\vec{\theta} \, | \, \mathrm{H}) \, \mathcal{L}(\mathbf{d} \, | \vec{\theta}) d\vec{\theta}.
\end{equation} 
This marginal likelihood is often called the evidence since it summarizes the likelihood of obtaining the data given the hypothesis over the entire prior distribution. We can compare the evidences between different hypotheses as a way to see which hypothesis offers a better explanation of the data. The larger the evidence, the better the hypothesis predicts the data. In most cases this integral is virtually intractable through analytic means. For this reason it is useful to consider Eq.~\ref{eqn:marg_likelihood} as a prior-weighted average likelihood via the following expression
\begin{equation}
    \mathcal{Z}(\mathbf{d} \, \, \mathrm{H}) = \frac{\int \pi(\vec{\theta}\, | \, \mathrm{H}) \mathcal{L}(\mathbf{d} \, | \, \vec{\theta}, \mathrm{H}) d\vec{\theta}}
                                                    {\int \pi(\vec{\theta} \, | \, \mathrm{H}) d\vec{\theta}} = \langle \mathcal{L}(\mathbf{d} \, | \, \vec{\theta}, \mathrm{H})_{\pi(\vec{\theta} \, | \, \mathrm{H})}.
\end{equation}
Here $\langle \mathcal{L}(\mathbf{d} \, | \, \vec{\theta}, \mathrm{H})_{\pi(\vec{\theta} \, | \, \mathrm{H})}$ denotes taking the average of the likelihood with respect to the measure defined by the prior distribution $\pi(\vec{\theta} \, | \, \mathrm{H})$. The marginal likelihood could be approximated via a Monte Carlo simulation across the prior distribution where we then calculate the likelihood at every point in the parameter space. We then take the average of our likelihood to estimate the evidence. In practice, this too is impractical in realistic astrophysical applications~\cite{wall2012practical}. In practice we also often deal with the logariithm of the likelihood since the likelihood is often too small to represent on a computer.

When comparing two competing hypotheses it is possible to use the likelihood ratio of the marginal likelihood to test which hypothesis is more probable. This likelihood ratio test is known as the Bayes factor. For two hypotheses, $\mathrm{H}_A$, and $\mathrm{H}_B$, the Bayes factor is defined as
\begin{equation}
    \mathcal{B}^{\mathrm{H}_A}_{\mathrm{H}_B} \equiv \frac{\mathcal{Z}(\mathbf{d} \, \, \mathrm{H}_A)}{\mathcal{Z}(\mathbf{d} \, \, \mathrm{H}_B)}
\end{equation}
The Bayes factor can be converted into a posterior odds ratio via:
\begin{equation}\label{eqn:odds_ratio}
    \mathcal{O}^{\mathrm{H}_A}_{\mathrm{H}_B} = \mathcal{B}^{\mathrm{H}_A}_{\mathrm{H}_B} \times \frac{\pi(\mathrm{H}_A)}{\pi(\mathrm{H}_B)}.
\end{equation}
In this equation, $\mathcal{O}^{\mathrm{H}_A}_{\mathrm{H}_B}$ represents the odds that hypothesis $\mathrm{H}_A$ is preferred over hypothesis $\mathrm{H}_B$ after the observation of the data. This is called the posterior odds ratio. The ratio $\pi(\mathrm{H}_A) / \pi(\mathrm{H}_B)$ represents our prior odds ratio, that is, how much more did we believe that hypothesis $\mathrm{H}_A$  was preferred over hypothesis $\mathrm{H}_B$  before observing the data. The prior odds ratio gives us a statement of what level of Bayes factor we would require before we begin to change our minds about the odds of hypothesis $\mathrm{H}_B$ being better supported in the data than hypothesis $\mathrm{H}_A$. It is considered good practice to state prior probabilities at the outset of an experiment including prior odds ratios on hypotheses~\cite{hobson2010bayesian}. Choice of prior probabilities on hypotheses are subjective but should not be considered arbitrary since they represent decisions in experimental design. An uninformative prior on each hypothesis, indicating no prior preference for either hypothesis, would then set the prior odds ratio to unity.

If there are only two hypotheses being considered then an odds ratio can be converted into a probability of one hypothesis over another hypothesis through the following expression~\cite{read2006encyclopedia}:
\begin{equation}\label{eqn:probability_odds_ratio}
    \mathcal{P} \left(\mathrm{H}_A \, | \, \mathbf{d}\right) = \frac{\mathcal{O}^{\mathrm{H}_A}_{\mathrm{H}_B}}{1 + \mathcal{O}^{\mathrm{H}_A}_{\mathrm{H}_B}}.
\end{equation}
Here the probability of hypothesis $\mathrm{H}_A$ after observation of the data is given as $p(\mathrm{H}_A \, | \, \mathbf{d})$. This is denoted as the posterior probability of the hypothesis $\mathrm{H}_A$. Since there are only two possible hypotheses the posterior probability of $\mathrm{H}_B$ is $p(\mathrm{H}_B)) \equiv 1 - p(\mathrm{H}_A))$. In Fig.~\ref{fig:log_odds_v_probability} we compare the logarithm of the odds ratio with the posterior probability for a hypothesis. When the odds ratio is unity the probability of one hypothesis versus another is $0.5$. Furthermore, we can also make a mapping of this probability to a single-tailed z-score of a Gaussian distribution. This is the familiar test statistic $\sigma$ used in physics (see Chapter 2 for an example of the probability that transient detector noise could generate an event as loud as or louder than GW150914). Specifically, the conversion from probability to this statistic is given by $\mathrm{z-score} = -\sqrt{2} \mathrm{erfc}^{-1}(2p)$, where $\mathrm{erfc}^{-1}$ is the inverse complementary error function and $p$ is the probability value\footnote{In Chapter 2 we converted p-values to single-sided Gaussian standard deviation scores via the inverse error function. The formulation used here is equivalent.}. A z-score of $0$ ($0 \sigma$) indicates a $50\%$ probability, while a z-score of $5$ is $\sim$ $10^{-7}$ probability. This z-score is Fig~\ref{fig:log_odds_v_z_score} shows how the z-score varies with natural logarithm of the odds ratio.

In cases where there are more than hypotheses available, we set one model as the fiducial model such that all Bayes factors are calculated relative to this fiducial hypothesis ($\mathrm{H}_{\mathrm{fiducial}}$)~\cite{read2006encyclopedia}. Then posterior probabilities for individual hypotheses can be calculated as
\begin{equation}\label{eqn:posterior_prob_multi}
    \mathcal{P} \left(\mathrm{H}_i \, | \, \mathbf{d}\right) = \frac{\mathcal{B}^{\mathrm{H}_i}_{\mathrm{H}_{\mathrm{fiducial}}}}
                                              {\sum^{\limit \mathrm{N}}_{\limit j=1} \mathcal{B}^{\mathrm{H}_j}_{\mathrm{H}_{\mathrm{fiducial}}}}.
\end{equation}
Here the summation is over all N available hypotheses. This posterior probability for an individual hypotheses is often useful to calculate to compare how informative any individual hypothesis is relative to all available models. In practice it is often difficult to test a large set of hypotheses, but~\cite{ligo2019model} is an example of testing a large set of hypotheses in the field of gravitational wave astronomy.

As~\cite{read2006encyclopedia} notes we are often not just concerned with the Bayes factors and posterior probabilities on hypotheses but we also want to learn from the inference on parameters is conditional on each of these hypotheses. We can inspect each of these posterior probabilities on parameters for each hypotheses in isolation or we can take the Bayesian model averaging approach of~\cite{kass1995bayes}. The basis of Bayesian model averaging involves coherently combining the parameter inference for common parameters from many different hypotheses. To do so we calculate the posterior probability for each hypothesis based on the Bayes factor (or posterior odds ratio if the prior odds ratios for hypotheses are not identical) as in Eq.~\ref{eqn:posterior_prob_multi}. Next, we reweight the marginal posterior probabilities for common parameters for each hypothesis by the posterior probability of the hypothesis. Finally, we sum over, or marginalize, over these marginal posterior probabilities of common parameters given each hypothesis to get a combined inference on the common parameters given all available hypotheses. This technique is used in Bayesian cosmological modeling and is known as multi-model parameter inference~\cite{hobson2010bayesian}. We present the marginalization procedure for a parameter $\Delta$ below
\begin{equation}\label{eqn:BMA}
    \mathcal{P} \left( \Delta \, | \, \mathbf{d}) = \sum_{\limit i=1}^{\limit \mathrm{N}}  \mathcal{P} \left(\mathrm{H}_i \, | \, \mathbf{d}\right) \times \mathcal{P} \left(\Delta \, | \, \mathbf{d}, \mathrm{H}_i\right).
\end{equation}
Here the summation is over all N available hypotheses. If we have a continuous ``hyper-parameter'' that connects the hypotheses we are testing then the summation becomes an integration over this ``hyper-parameter''. 

In addition to testing hypotheses on one particular datum $\mathbf{d}$ it is possible to combine the inference of data, $\left\{\mathbf{d}_1, \mathbf{d}_2, \ldots, \mathbf{d}_\mathrm{N}\right\}$. If we are testing the same prior distribution and each datum is statistically independent, then the Bayes factor for multiple observations can be combined through multiplication~\cite{del2013demonstrating}. We temporarily suppress notation on hypotheses, and adopt the notation of~\cite{ly2018replication} for Bayes factors from multiple observations to give the combined Bayes factor as,
\begin{equation}\label{eqn:multi_bayes_factors}
   \mathcal{B}(\mathbf{d}_1, \mathbf{d}_2, \ldots, \mathbf{d}_{\mathrm{N}}) = \mathcal{B}(\mathbf{d}_1) \times \mathcal{B}(\mathbf{d}_2) \times \ldots \times \mathcal{B}(\mathbf{d}_N). 
\end{equation}
Here $\mathcal{B}(\mathbf{d}_1, \mathbf{d}_2, \ldots, \mathbf{d}_{\mathrm{N}})$ represents the Bayes factor over many observations. Even if the Bayes factor for a particular hypothesis is not statistically significant for any individual observation $\mathbf{d}_i$, we can accumulate evidence over many observations to reach a clearer conclusion about how well supported a hypothesis is by the many observations.

We can also update our posterior distributions on parameters over many events. Here we follow~\cite{dpac_technical} and consider
the updated marginal posterior probability $\mathcal{P} \, \left(\Delta \, | \, \mathrm{H}, \mathbf{d}_1, \ldots, \mathbf{d}_N, \right)$ on a parameter $\Delta$ after N observations
\begin{equation}
    \mathcal{P} \, \left(\Delta \, | \, \mathrm{H}, \mathbf{d}_1, \ldots, \mathbf{d}_N, \right) = \frac{\pi (\Delta \, | \, \mathrm{H} )}{c} \mathcal{L} (\mathbf{d}_1, \ldots, \mathbf{d}_N) \, | \, \Delta , \mathrm{H}). 
\end{equation}
Here we have a prior $\pi (\Delta \, | \, \mathrm{H} )$ representing our belief on the parameter $\Delta$ over the entire population of our observations. Our likelihood function $\mathcal{L}$ is separable if all events are statistically independent. This gives
\begin{equation}
    \mathcal{P} \, \left(\Delta \, | \, \mathrm{H}, \mathbf{d}_1, \ldots, \mathbf{d}_N, \right) = \frac{\pi (\Delta \, | \, \mathrm{H} )}{c} \mathcal{L} (\mathbf{d}_1  \, | \, \Delta , \mathrm{H}) \times \mathcal{L} (\mathbf{d}_2  \, | \, \Delta , \mathrm{H}) \times \ldots \mathcal{L} (\mathbf{d}_N  \, | \, \Delta , \mathrm{H}).
\end{equation}
We can attain the likelihood $\mathcal{L}(\mathbf{d}_i  \, | \, \Delta , \mathrm{H})$ via Bayes' theorem by dividing the marginal posterior distribution $\mathcal{P}' (\Delta \, | \, \mathbf{d}_i, \mathrm{H}) / \pi'(\Delta \, | \, \mathrm{H})$. We use a prime notation here on the marginal posterior distribution and marginal prior distribution on the parameter $\Delta$ to denote the distributions from observation of $\mathbf{d}$. Marginal posterior distributions on parameters can be approximated from an Monte Carlo simulation or a Markov-Chain Monte Carlo simulation. This procedure is also amenable to multi-model inference so that our parameter inference is not completely dependent on any one particular parameter hypothesis.

We have so far treated the Bayesian evidence and Bayes factor as exact quantities that can be estimated exactly. We now consider error propagation and uncertainty estimation for Bayesian evidences and Bayes factors.

\section{Uncertainty propagation in Bayes factor estimation}\label{sec:practical_bayes}
When comparing hypotheses practically we must confront the fact that it is impossible to calculate the evidence (or log evidence) analytically and so we often turn to Markov Chain Monte Carlo techniques to approximate the evidence~\cite{hobson2010bayesian}. These techniques will often give a point-estimate or mean value estimate of the logarithm of the evidence with some estimation of the uncertainty in the estimate. In our treatment we consider a model of the estimation of the logarithm of the evidence as a Gaussian distribution in log-likelihood. This distribution has mean $\mu_{\widehat{\mathrm{ln} \, \mathcal{Z}}}$ given by a Markov-Chain Monte Carlo (MCMC) method's point-estimate, and standard deviation $\sigma_{\widehat{\mathrm{ln} \, \mathcal{Z}}}$ representing systematic or statistical uncertainties in the MCMC method. Thus the logarithm of the evidence can be represented as 
\begin{equation}\label{eqn:p_log_z}
    p(\widehat{\mathrm{ln} \, \mathcal{Z}}) = \left(\frac{1}{\sqrt{2 \pi \sigma_{\widehat{\mathrm{ln} \, \mathcal{Z}}}}} \right) \mathrm{exp} \left \{-\frac{\left(\mathrm{ln} \, \mathcal{Z} - \mu_{\widehat{\mathrm{ln} \, \mathcal{Z}}}\right)^2} {2 \sigma_{\widehat{\mathrm{ln} \, \mathcal{Z}}}}  \right\}.
\end{equation}

Since the Bayes factor is the ratio of two evidences, the logarithm of the Bayes factor is the difference of the logarithms of two evidences. Here we suppress notation on hypothesis $\mathrm{H}_A$ and hypothesis $\mathrm{H}_B$, instead calling them $A$ and $B$,
\begin{equation}\label{eqn:log_bayes_factor}
    \mathrm{ln} \, \mathcal{B}^A_B = \mathrm{ln} \, \mathcal{Z}_{\mathrm{A}} - \mathrm{ln} \, \mathcal{Z}_{\mathrm{B}}.
\end{equation}
However, since we treat $\mathrm{ln} \, \mathcal{Z}_{\mathrm{A}}$ as a random variable we must deal with the uncertainty in $\widehat{\mathrm{ln} \, \mathcal{Z}_{\mathrm{A}}}$. The logarithm of the Bayes factor then becomes the difference between two probability distribution functions. This can be solved via convolution and has been solved for the Gaussian case~\citep{bromiley2003products}. From \cite{bromiley2003products}, we can express $\widehat{\mathrm{ln} \, \mathcal{B}^A_B}$ as a Gaussian distribution function with mean $\mu_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B}} = \mu_{\widehat{\mathrm{ln} \, \mathcal{Z}_A}} - \mu_{\widehat{\mathrm{ln} \, \mathcal{Z}_B}}$ and standard deviation $\sigma_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B}} = \sqrt{\sigma_{\widehat{\mathrm{ln} \, \mathcal{Z}_A}}^2 + \sigma_{\widehat{\mathrm{ln} \, \mathcal{Z}_B}}^2 }$. This gives us the following expression for the distribution function on the logarithm of the Bayes factor:
\begin{equation}\label{eqn:p_log_b}
    p(\widehat{\mathrm{ln} \, \mathcal{B}^A_B}) = \left(\frac{1}{\sqrt{2 \pi \sigma_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B}}}} \right) \mathrm{exp} \left \{-\frac{\left(\widehat{\mathrm{ln} \, \mathcal{B}^A_B} - \mu_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B})}\right)^2} {2 \sigma^2_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B}}}  \right\}.
\end{equation}

The expression in Eq.~(\ref{eqn:p_log_b}) is a Gaussian distribution function in $\widehat{\mathrm{ln} \, \mathcal{B}^A_B}$, but we often prefer to know the estimate on $\mathcal{B}^A_B$ and so we must transform coordinates. This transformation of coordinates, is a well-known distribution called the log-normal distribution. We write out our log-normal probability distribution function for $\widehat{\mathcal{B}^A_B}$ as
\begin{equation}
    p(\widehat{\mathcal{B}^A_B}) = \frac{1}{\widehat{\mathcal{B}^A_B} \, \sigma_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B}}} \frac{1}{2\pi} \mathrm{exp} \left \{-\frac{\left(\mathrm{ln} \, \widehat{\mathcal{B}^A_B} - \mu_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B})}\right)^2} {2 \sigma^2_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B}}}  \right\}.
\end{equation}
It is worth noting that for a sufficiently small standard deviation on the logarithm of the Bayes factor, the probability of the distribution function will look approximately Gaussian in shape. For the log-normal Bayes factor distribution the median value of the distribution is identical to the point-estimate Bayes factor, $\mathcal{B}^A_B = \mathrm{exp} \left[\mathrm{ln} \, \mathcal{Z}_{\mathrm{A}} - \mathrm{ln} \, \mathcal{Z}_{\mathrm{B}} \right]$. The expectation value (mean) of this log-normal distribution is always right of the median, while the mode of the distribution is left of the median. Large standard deviations on the logarithm of the evidence will create very long tails for the distribution of the Bayes factor, which makes decision-making based on Bayes factors more risky. Studies that use this estimation of the Bayes factor should consider limiting the error on the logarithm of the evidence to mitigate propagating large error to the Bayes factor.

In Sec.~\ref{} we gave an equation~\ref{eqn:multi_bayes_factors} for combining the evidence from a series of observations. In Eq.~\ref{eqn:multi_bayes_factors} we represented the combined Bayes factor across observations as the product of the Bayes factor from each observation. If we consider the logarithm of the Bayes factor then combining the combined logarithm of the Bayes factor is the sum of the logarithm of the Bayes factor from each observation. In this section we have modeled the logarithm of the Bayes factor as a Gaussian distribution, and so the combined logarithm of the Bayes factor across observations is the sum of a series of Gaussian distributions, which itself is a Gaussian distribution~\cite{bromiley2003products}. The combined logarithm of the Bayes factor across many observations is the Gaussian distribution described by
\begin{equation}
    p(\widehat{\mathrm{ln} \, \mathcal{B}^A_B}) \left(\mathbf{d}_1, \mathbf{d}_2, \ldots, \mathbf{d}_N \right) = \mathcal{N}\left(\mu = \sum \limits_{i=1}^{N} \mu_i, \, \, \sigma = \sqrt{\sum \limits_{i=1}^{N} \sigma_i^2}\right).
\end{equation} 
Here $p(\widehat{\mathrm{ln} \, \mathcal{B}^A_B}) \left(\mathbf{d}_1, \mathbf{d}_2, \ldots, \mathbf{d}_N \right)$ is the combined Bayes factor of hypothesis A vs hypothesis B. Here $\mu_i$ is the point-estimate log-Bayes factor from observation $\mathbf{d}_i$, and $\sigma_i$ is the standard deviation of the log-Bayes factor from observation $\mathbf{d}_i$. If the $\mu_i$ and $\sigma_i$ are all comparable across data, then it is useful to note that the mean value of the combined logarithm of the Bayes factor estimate tends to grow linearly for $N$ observations, while the standard deviation of the combined logarithm of the Bayes factor grows as $\sim \sqrt{N}$. And so, even if we cannot reduce $\sigma_i$ very well for individual observations, in the long-run of observations we may expect the combined logarithmic Bayes factor to tend in a direction of statistical significance where a decision on the hypothesis can be confidently made. If the combined logarithm of the Bayes factor constantly oscillates around $0$ over many observations then the error term $\sigma$ will overcome the mean-value $\mu$ and yield asymptotic decisions statistical significance of hypothesis A relative to hypothesis B due unconstrained growth in the uncertainty.

To illustrate this growth in Bayes factor uncertainty we consider a toy-model where three estimators of the logarithm of the Bayes factor for hypothesis $\mathrm{H}_A$ and hypothesis $\mathrm{H}_B$. We denote these three log Bayes factor estimators as $LBFE-1$, $LBFE-2$, and $LBFE-3$. We consider $400$ observations, where the true value of the log Bayes factor is $0.05$ for every observation. This is only modest evidence for hypothesis $\mathrm{H}_A$ for each observation, but the evidence accumulates over several observations. After $400$ observations the combined log Bayes factor is $\400 \times 0.05 = 20$, which is a combined Bayes factor of $4.8 \times 10^8$. This would provide very large evidential support for hypothesis $\mathrm{H}_A$ (see Table~\ref{table:bayes_factor_interpretation} for rule-of-thumb statistical significance interpretation). The first Bayes factor estimator $LBFE-1$ measures an unbiased estimate the log Bayes factor for each observation such that $\mu_i = 0.05$ and $\sigma_i = 0.05$. The second Bayes factor estimator $LBFE-2$ also has an unbiased estimate the log Bayes factor for all observations such that $\mu_i = 0.05$, but has trouble getting a good error estimate, measuring with uncertainty $\sigma_i = 0.3$ in the log Bayes factor for each observation. Finally, the third Bayes factor estimator $LBFE-3$ uses a method of Bayes factor estimation that is unintentionally biased such that they measure $\mu_i=0$ for all estimates, but their estimator gives a statistical uncertainty of $\sigma_i = 0.05$. Fig.~\ref{AAAA} shows the behavior of these three Bayes factor estimators and their uncertainty at the $90 \%$ confidence interval. After $200$ observations, the true combined log Bayes factor is $10$, and so we have that $LBFE-1$ has estimated a combined log Bayes factor of ($8.8$, $10$, $11.2$), while $LBFE-2$ has estimated a combined log Bayes factor of ($3$, $10$, $17$), and finally $BFE-3$ has estimated a combined log Bayes factor of ($-7$, $0$, $7$ )  at the ($5^{\mathrm{th}}$, $50^{\mathrm{th}}$, $95^{\mathrm{th}}$) percentiles respectively. Here, $LBFE-1$ is clearly the best estimator of the evidence, however $LBFE-2$ is also tracking positive support for hypothesis $\mathrm{H}_A$. We see that after $200$ observations $LBFE-3$ has an unresolved Bayes factor with no evidential support for either hypothesis. The uncertainty confidence intervals for $LBFE-2$ and $LBFE-3$ still overlap however. After $400$ observations we find $LBFE-1$ has measured ($18.3$, $20$, $21.6$), $LBFE-2$ has measured ($29.9$, $20$, $10$), and $LBFE-3$ has measured ($-9.9$, $0$, $9.9$). Finally, after $400$ observations, the $90 \%$ confidence intervals of $LBFE-2$ and $LBFE-3$ no longer overlap and $LBFE-1$ and $LBFE-2$ have decisive levels of statistical significance to give support to $\mathrm{H}_A$. Meanwhile, $LBFE-3$ has accumulated no relative evidence between hypotheses and the uncertainty has become extremely large. 

If we seek to use Bayes factors to make decisions on the credibility of hypotheses in gravitational wave astronomy we should seek to use the most accurate and unbiased methods for their estimation. Reducing the bias and variance of our Bayes factor estimators will play an important role in our ability to discriminate theories in physics that are supported by the data from those that are not. We now move on to practical methods for estimating the Bayesian evidence, Bayes factors, and their uncertainties via Markov-Chain Monte Carlo methods.

\section{The Thermodynamic Integration Method for Estimating the Bayesian Evidence}
The first Markov-Chain Monte Carlo (MCMC) method that we consider for Bayesian hypothesis testing is the thermodynamic integration method. Many MCMC samplers use a chain of multiple temperatures to simulate annealing gradually guiding the MCMC sampler from the prior distribution to the posterior distribution~\citep{emcee, vousden:2016, doi:10.1143/PTPS.157.317, B509983H}. These multiple temperatures are helpful for achieving a global optimization routine to find the largest modes of the posterior distribution. In addition to finding the modes of the posterior distribution, this method is also useful for estimating the logarithm of the evidence. These method of thermodynamic integration that we follow here is called the power-posterior method~\cite{lartillot2006computing, friel2008marginal}. Each temperature describes a tempered posterior distribution, which is called a power-posterior because it can be written as
\begin{equation}
    \mathcal{P}\left(\vec{\theta}|\mathbf{d}, H\right)_\beta \propto \pi\left(\vec{\theta} | H\right) \mathcal{L}\left(\mathbf{d} | \vec{\theta}, H\right)^\beta.
\end{equation}\label{eq:power_posterior}
Here $\mathcal{P}\left(\vec{\theta}|\mathbf{d}, H\right)_\beta$ is the power-posterior. The likelihood is raised to the power of the inverse-temperature $\beta$. The prior distribution is left identical to the standard, untempered prior distribution usesd in hypothesis testing and paramter inference. For a value of $\beta$ = $0$ the power-posterior is equivalent to the prior distribution, while for a value of $\beta$ = $1$ the power-posterior is equivalent to the posterior distribution. The normalization constant for the power-posterior in Eq.~\ref{eq:power_posterior} is the evidence for that power-posterior, given as $\mathcal{Z}(\mathbf{d} \, | \ ,H)_\beta \equiv \int \pi\left(\vec{\theta} \, | \, \mathrm{H}\right) \mathcal{L}\left(\mathbf{d} | \vec{\theta}, \mathrm{H}\right)^\beta d\vec{\theta}$.

From these power-posterior distributions we can use a thermodynamic integration method~\citep{lartillot2006computing,friel2008marginal} to estimate the logarithm of the evidence. For the derivation and discussion of this thermodynamic integration method we follow \citep{annis2019thermodynamic}
We begin by considering the following expression implied by the 2nd Fundamental theorem of Calculus:
\begin{equation}\label{eqn:ti_identity}
    \mathrm{ln} \, \mathcal{Z}_{\beta=1}\left(\mathbf{d}\right) - \mathrm{ln} \, \mathcal{Z}_{\beta=0}\left(\mathbf{d}\right) = \int^1_0 \left(\frac{d\left(\mathrm{ln} \, \mathcal{Z}_\beta \left(\mathbf{d}\right) \right)}{d\beta}\right) \, d\beta = \int^1_0 \frac{1}{\mathcal{Z}_\beta \left(\mathbf{d}\right)} \frac{d \, \mathcal{Z}_\beta \left(\mathbf{d}\right)}{d\beta} d\beta.
\end{equation}
For a properly normalized prior, $\pi(\vec{\theta}$, $\mathrm{ln} \, \mathcal{Z}_{\beta=0} \left(\mathbf{d}\right) = 0$. This leaves the marginal likelihood at $\beta=1$ that we are interested in which is the untempered $\mathrm{ln} \, \mathcal{Z} \left(\mathbf{d}\right)$. Now we can expand Eq.~\ref{eqn:ti_identity} as:
\begin{equation}
    \mathrm{ln} \, \mathcal{Z} \left(\mathbf{d}\right) = \int_0^1 \frac{\int \frac{d}{d\beta} \left[\pi\left(\vec{\theta}\right) \, \mathcal{L} \left(\mathbf{d}|\vec{\theta} \right)^\beta \right]\, d\vec{\theta} d\vec{\theta}}{\int \pi\left(\vec{\theta}\right) \, \mathcal{L}\left(\mathbf{d}|\vec{\theta} \right)^\beta \, d\vec{\theta}}.
\end{equation}
Suppressing notation on $\theta$ and $\mathbf{d}$, for clarity, we can arrive at the following expression by taking the derivative in the numerator and arriving at:
\begin{equation}
    \mathrm{ln} \, \mathcal{Z} = \int^1_0 \frac{\int \pi \, \, \, \left(\mathrm{ln} \, \mathcal{L}\right) \, \, \, \mathcal{L}^{\beta} d\theta}{\int \pi \mathcal{L}^{\beta} d\theta} d\beta.
\end{equation}
Using Bayes' theorem we can replace the numerator and denominator with $\mathcal{P}_\beta = \pi \mathcal{L}^\beta / \mathcal{Z}_\beta$ to get:
\begin{equation}
    \mathrm{ln} \, \mathcal{Z} = \int^1_0 \frac{\int \mathcal{P}_\beta \, \left(\mathrm{ln} \, \mathcal{L}\right) d\theta}{\int \mathcal{P}_\beta   d\theta} d\beta = \int^1_0 \langle \mathrm{ln} \, \mathcal{L} \rangle_{\mathcal{P}_\beta} \, d\beta,.
\end{equation}
Therefore, the logarithm of the evidence is given by the one dimensional integral in Eq.~(\ref{eq:thermoint}).
Here $\langle \mathrm{ln} \, \mathcal{L} \rangle_{\mathcal{P}_\beta}$ represents the average untempered log-likelihood under the measure described by the power-posterior distribution at $\beta$. This is the average untempered log likelihood when drawing random samples from the power-posterior distribution at $\beta$. We suppress this notation to write $\langle \mathrm{ln} \, \mathcal{L} \rangle_{\mathcal{P}_\beta} \equiv \langle \mathrm{ln} \, \mathcal{L} \rangle_\beta$. With the thermodynamic integration method we have reduce a potentially large $N$ dimensional integral into a one-dimensional integral. Fortunately, this method is an ubiased estimator of the evidence provided that samples of $\langle \mathrm{ln} \, \mathcal{L} \rangle_\beta$ can be drawn in an unbiased manner from power-posteriors~\citep{carlson2016partition}.

It is also convenient to describe additional derivatives of the thermodynamic integrand. In general, $\mathrm{n}^{\mathrm{th}}$ derivatives of the form $\mathrm{ln} \, \mathcal{Z}$ can be solved by referring to Eq.~$0.435$ of~\cite{gradshteyn2015table}\footnote{Note that the solution in~\cite{gradshteyn2015table} has a minor typo, which we correct here.}:
\begin{equation}\label{eqn:gradshteyn_derivatives}
    \frac{d^n}{d\beta^n}\left( \mathrm{ln} \, \mathcal{Z} \right) = \sum_{k=1}^{n} \frac{(-1)^{(k+1)} {{n}\choose{k}}}{k \mathcal{Z}^k} \frac{d^n}{d\beta^n} \left(\mathcal{Z}^k\right).
\end{equation}
The first derivative, $n=1$, we have already solved as $\langle \mathrm{ln} \, \mathcal{L} \rangle_\beta$. The next derivative, $n=2$, was found in~\cite{friel2014improving} as $\mathrm{Var}(\mathrm{ln} \, \mathcal{L})_\beta$ = $\langle (\mathrm{ln} \, \mathcal{L})^2\rangle_beta - \langle \mathrm{ln} \, \mathcal{L} \rangle^2_\beta$. This is the variance of the untempered log likelihood samples when drawn from the power-posterior at $\beta$. We solve the next derivative, $n=3$, as:
\begin{equation}\label{eqn:third_ti_deriv}
    \frac{d^3}{d\beta^3}\left( \mathrm{ln} \, \mathcal{Z}\right) = \langle \left(\mathrm{ln} \, \mathcal{L} \right)^3\rangle_\beta + 2 \langle \mathrm{ln} \, \mathcal{L} \rangle^3_\beta - 3 \langle \left(\mathrm{ln} \, \mathcal{L} \right)^2\rangle_\beta \langle \mathrm{ln} \, \mathcal{L}\rangle_\beta.
\end{equation}
In practice, this third derivative is not computationally stable for many applications in gravitational wave astronomy where the log likelihood can be $(\sim\mathcal{O}(10^{-7})$. However, we observe that the pattern of derivatives $\mathrm{ln} \, \mathcal{Z}$ with respect to $\beta$ follow the pattern of the $\mathrm{n}^{\mathrm{th}}$ cumulants~\citep{kardar2007statistical} of the power-posterior distribution at $\beta$~\cite{carlson2016partition}, see Table~\ref{table:cumulants}. In fact, $\mathcal{Z}$ describes a partition function for the posterior distribution~\citep{carlson2016partition, lamont2019correspondence}. This cumulant property is helpful because it can make computation of values of higher order derivatives more numerically stable since cumulants of order $\ge 2$ are shift-invariant~\cite{kardar2007statistical}. We can make the transformation of variables, $\widetilde{\mathrm{ln} \, \mathcal{L}} \equiv \mathrm{ln} \, \mathcal{L} - \mathrm{ln} \, \mathcal{L}_{\mathrm{max}}$ for every power-posterior before computing the numerical value of higher order derivatives of $\mathrm{ln} \, \mathcal{Z}$. We have tested this transformation rule on higher order derivatives and found it to be both accurate and numerically stable, confirming the cumulant properties of the derivatives. However, we have also found that the power-posterior distributions found through using the parallel-tempered \emph{emcee} sampler~\citep{emcee,vousden:2016} may not be accurate enough to permit calculation of derivatives higher than order $3$ in all cases.

In Fig.~\ref{fig:gooseneck_lienar} we show the thermodynamic integrand and the next two derivatives for a gravitational wave analysis that uses $51$ temperatures. We also show the thermodynamic integrand and the next two derivatives on a logarithmic scale in $\beta$ in Fig.~\ref{fig:gooseneck_log} so that the curvature of the thermodynamic integrand is easier to see. In practice, plots like Figs.~\ref{fig:gooseneck_linear}, \ref{fig:gooseneck_log} are helpful to inspect for places where the integrand may not be well sampled in $\beta$ and hence require additional inverse-temperatures~\cite{liu2016evaluating, de2011free, de2013comparison} to achieve an accurate estimate of the logarithm of the evidence. Of particular note is the instability in the second (third) subplot of Fig.~\ref{fig:gooseneck_log} where the second (third) derivative is not perfectly smooth in $\beta$. We expect the thermodynamic integrand to be smooth and monotonically increasing as $\beta$ goes from $0$ to $1$\cite{annis2019thermodynamic}, although there is some numerical instability at $\beta \sim 10^{-9}$. The other derivatives of the thermodynamic integrand should also be smooth. This instability implies, the need for a better tempering sampler or bias-corrective terms in the sampling such as those found in the multi-tempering samplers of~\cite{oates2017control,evans2019thermodynamic}. The instability in Figs.~\ref{fig:gooseneck_linear}, \ref{fig:gooseneck_log} is very slight however and we would not expect an effect like this to significantly impact the Bayes factor estimation. We now move on to numerical quadrature routines to numerically estimate the thermodynamic integral from a finite set of inverse-temperatures $\beta$.

\subsection{Numerical Quadrature}
The thermodynamic integral in Eq.~(\ref{eq:thermoint}) can be estimated through numerical quadrature rules such as the trapezoidal rule or Simpson's rule. Optimal placements of inverse-temperatures $\beta$ are not typically uniformly distributed between $0$ and $1$~\cite{annis2019thermodynamic}. Therefore it is helpful to consider integration rules that do not depend on equally spaced abscissa ($\beta$ in the context of thermodynamic integration). A polynomial interpolant that does not make of equally spaced abscissa is the Newton's divided difference interpolant, see~\cite{brun1953generalization, selmer1958numerical, abramowitz1965handbook} for how to construct these polynomials. We can then integrate these interpolants to create quadrature rules.

First we consider the trapezoidal rule which in the context of thermodynamic integration is
\begin{equation}
    \widehat{\mathrm{ln} \, \mathcal{Z}}_{\mathrm{Trapz}} = \sum_{i=0}^{N_\beta-1} \frac{1}{2} \left(\beta_{i+1} - \beta_i \right) \left(\langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i+1}} + \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i}} \right)
\end{equation}
Here $N_\beta$ represents the number of $\beta$ being summed over in the integration estimation. The error correction term to the trapezoidal rule can be found by integrating the next-to-leading order Taylor polynomial~\citep{abramowitz1965handbook}, yielding:
\begin{equation}
    \widehat{\mathrm{ln} \, \mathcal{Z}}_{\mathrm{Trapz \, +}} \approx \widehat{\mathrm{ln} \, \mathcal{Z}}_{\mathrm{Trapz}} + \sum_{i=0}^{N_\beta-1} -\frac{1}{12} \left(\beta_{i+1} - \beta_i \right)^2 \left(f'(\beta_{i+1}) - f'(\beta_{i}) \right).
\end{equation}
Here $f'(\beta_i)$ represents the second derivative of $\mathrm{ln} \, \mathcal{Z}$ with respect to $\beta$. It was found in~\cite{friel2014improving} that this corresponds to the variance of the untempered log likelihood as drawn from the power-posterior at $\beta_i$.

Simpson's rule for unequally spaced abscissa under Newton's divided difference interpolation~\citep{easa1988area} is:
\begin{equation}
    \widehat{\mathrm{ln} \, \mathcal{Z}}_{\mathrm{Simps}} = \sum_{\mathrm{i\, is\, even}, \, \, i=0}^{N_\beta-2} \frac{h_i + h_{i+1}}{6} \left [ A \, \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i}} + B \, \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i+1}} + C \, \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i+2}}\right ],
\end{equation}
for the expressions:
\begin{equation}
\begin{array}{lll}
     A &= \frac{(2h_i - h_{i+1})}{h_i} \\ \\ 
     B &= \frac{(h_i+h_{i+1})^2}{h_i h_{i+1}} \\ \\
     C &= \frac{(2h_{i+1} - h_i)}{h_{i+1}}. \\ \\
\end{array}
\end{equation}
Here  $h_i \equiv \beta_{i+1} - \beta_i$, and $h_{i+1} \equiv \beta_{i+2} - \beta_{i+1}$. The error corrective term for Simpson's rule can thus be solved in the same manner as for the trapezoidal rule and we find:
\begin{equation}
    \widehat{\mathrm{ln} \, \mathcal{Z}}_{\mathrm{Simps \, +}} \approx \widehat{\mathrm{ln} \, \mathcal{Z}}_{\mathrm{Simps}} + \sum_{\mathrm{i\, is\, even}, \, \, i=0}^{N_\beta-2} \frac{1}{72}\left(\beta_{i+2} - \beta_{i} \right)^2 (\beta_i - 2\beta_{i+1} + \beta_{i+2})\frac{f''(\beta_{i+2}) - f''(\beta_i)}{\beta_{i+2} - \beta_i}.
\end{equation}
Here $f''(\beta_i)$ represents the third derivative of $\mathrm{ln} \, \mathcal{Z}$ with respect to $\beta$, which is in Eq.~\ref{eqn:third_ti_deriv}.

The cubic integration rule for unequally spaced abscissa under Newton's divided difference interpolation can be found in~\cite{brun1953generalization,selmer1958numerical,chambers1989estimating} or can be derived through the tools in~\cite{abramowitz1965handbook}. We present the form given in~\cite{chambers1989estimating}:
\begin{equation}
    \widehat{\mathrm{ln} \, \mathcal{Z}}_{\mathrm{cubic}} = \sum_{\mathrm{i\, is\, a \, multiple \, of \, 3}, \\ i=0}^{N_\beta-3} \frac{h_i + h_{i+1} + h_{i+2}}{12} \left [A \, \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i}} + B \, \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i+1}} + C \, \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i+2}} + D \, \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i+3}}\right ],
\end{equation}
for expressions:
\begin{equation}
\begin{array}{llll}
     A &= \frac{3h_i^2 -h_{i+1}^2 +h_{i+2}^2 +2 h_i h_{i+1} - 2h_i h_{i+2}}{h_i (h_i + h_{i+1})} \\ \\
     B &= \frac{(h_i + h_{i+1} + h_{i+2})^2 (h_i + h_{i+1} - h_{i+2})}{h_{i} h_{i+1} (h_{i+1} h_{i+2})} \\ \\
     C &= \frac{(h_i + h_{i+1} + h_{i+2})^2 (h_{i+1} + h_{i+2} - h_i)}{h_{i+1} h_{i+2} (h_i + h_{i+1})} \\ \\
     D &= \frac{h_i^2 - h_{i+1}^2 +3 h_{i+2}^2 - 2h_i h_{i+2} + 2 h_{i+1} h_{i+2}}{h_{i+2}(h_{i+1} + h_{i+2})}.\\ \\
\end{array}
\end{equation}
Here we have defined $h_i \equiv \beta_{i+1} - \beta_{i}$, $h_{i+1} \equiv \beta_{i+2} - \beta_{i+1}$, and $h_{i+2} \equiv \beta_{i+3} - \beta_{i+2}$.

We exercise caution in describing the thermodynamic integral through a higher order polynomial quadrature rule as the integrand may not be well interpolated by higher order polynomials. Thus there may be very little incentive for going to higher order polynomial rules as improved accuracy is not always guaranteed by going to higher order polynomial integration rules~\citep{epperson1987runge}. When the true value of the logarithm of the evidence is completely unknown we recommend comparing the estimates from all available quadrature rules to ensure consistency.

Future studies may make use of quadrature rules from Taylor series polynomials for unequally spaced abscissa, from ratios of Taylor series polynomials through the Pad$\textrm{\'e}$ approximant for improved accuracy~\citep{press1992pade}, or other interpolant functions. Improvement in numerical integration for thermodynamic integration may also be improved by focusing on increasing the number of inverse-temperatures $\beta$ and by improved placement of $\beta$.

\subsection{Monte Carlo Error Estimation}
Here we follow the discussion from~\cite{annis2019thermodynamic} for estimating the Monte Carlo error in the thermodynamic integral under a generic quadrature rule. The Monte Carlo error for thermodynamic integration is the uncertainty in the estimate of the integral due to only having a finite set of samples from a Markov-Chain Monte Carlo simulation. This uncertainty enters into the integration as an uncertainty in the $\langle \mathrm{ln} \, \mathcal{L} \rangle$. The variance of the thermodynamic integral estimator, $\widehat{\mathrm{ln} \, \mathcal{Z}}$, from Monte Carlo error can be found in two steps. First, calculate the thermodynamic integral for each sample of untempered log likelihoods drawn from the power-posterior at $\beta$. For $N$ samples drawn from each power-posterior this generates $N$ thermodynamic integral values. The quadrature rule for the integration is generic; we can use the trapezoidal rule, Simpson's rule, etc. From these $N$ integral values we take the sample variance and then divide by the number of samples $N$ to generate an estimate of the population variance of the thermodynamic integration. This population variance of the logarithm of the evidence represents a long-run estimate of the variance of the estimator. This variance can be represented as:
\begin{equation}
    \sigma^2_{\mathrm{MC}} = \frac{1}{N} \sigma^2_{\mathrm{sample}}.
\end{equation}
Here, $\sigma^2_{\mathrm{MC}}$ represents the Monte-Carlo variance for the thermodynamic integration estimator while $\sigma^2_{\mathrm{sample}}$ is the sample variance and $N$ represents the number of available samples. See Fig.~\ref{fig:ti_monte_carlo_error} for a visualization of this procedure.

Repeated runs where the random seed for the MCMC analysis was changed has shown that the variance estimate from presented in~\cite{annis2019thermodynamic} is a plausible confidence interval estimate for Monte Carlo error. This is potentially temperature-placement and hypothesis dependent and may require further investigation in the future.

\subsection{Convergence Error Estimation}\label{sec:evidence_convergence}
The procedure for estimating the marginal likelihood from power-posterior simulation requires that the power-posteriors all converge to a final stationary distribution~\cite{lartillot2006computing}. To do this we inspect the stability of the thermodynamic integral and integrand over the course of the MCMC analysis. To investigate the convergence of the evidence over the course of the MCMC analysis we must draw independent and identically distributed samples of the chains of the MCMC analysis at different intervals~\cite{annis2019thermodynamic}.

Gathering independent and identically distributed samples from a power-posterior can be done in \pycbc{}\ Inference by calculating the autocorrelation length of the MCMC chains of that power-posterior. In practice, \pycbc{}\ Inference calculates the autocorrelation length of all of the temperature chains and uses the largest posisble autocorrelation length as the autocorrelation length for all temperatures~\cite{biwer2019pycbc}. This is a safe and conservative practice for ensuring that samples drawn from the Markov Chain Monte Carlo simulation are not correlated. Thus, to track the thermodynamic integrand at various iterations in the MCMC simulation we divide the MCMC analysis into $12$ equally spaced segments. In practice any number of segments will do, but it is computationally intensive to sample more segments. The segments do not need to be equally spaced in MCMC iterations but we find equally spaced segments to be useful for inspecting the progression of the thermodynamic integrand. Using this number of segments, each segment is partitioned in half, where the first half is discarded as burn-in samples, and the autocorrelation length is calculated from the remaining half of the samples. Independent samples are drawn from this half of the segment by drawing a sample for every autocorrelation length. This is the generic procedure of the $\mathrm{n_{acl}}$ algorithm implemented in \pycbc{}\ Inference for drawing independent samples from the Markov chains~\cite{biwer2019pycbc}. The segmenting and partitioning procedure is shown in Fig.~\ref{fig:nacl_segments}.

Having drawn independent samples from $12$ segments of the MCMC analysis we can visually inspect the stability of the thermodynamic integrand at $12$ iterations in the MCMC analysis. We can also inspect the convergence of the thermodynamic integral. When the logarithm of the evidence has converged to $\mathcal{O}(10^{-2}$ accuracy, we usually consider the power-posteriors to have converged to their final stationary distribution. Figure~\ref{fig:integrand_convergence} shows the progression of the convergence of the thermodynamic integrand as a function of the MCMC iteration. The MCMC iteration denotes how far along the MCMC has progressed, where the final iteration value indicates where the MCMC analysis was terminated. Figure~\ref{fig:integral_convergence} shows the convergence rate of the thermodynamic integral as a function of the MCMC iteration for a variety of integration techniques.

Finally, the absolute value of the difference between the last two thermodynamic integration estimates from this segmenting procedure are then used for for the standard deviation of the error for the log evidence due to convergence error, $\sigma_{\mathrm{convergence}}$:
\begin{equation}
    \sigma_{\mathrm{convergence}} \sim  \, \mid \mathrm{ln} \, \mathcal{Z}_{\mathrm{partition \,} N} - \mathrm{ln} \, \mathcal{Z}_{\mathrm{partition \,} N-1} \mid
\end{equation}
This provides a rough estimate for estimating the consequences of potentially terminating the MCMC analysis too early.

During the development of this technique a similar technique based on a moving-block bootstrap method was developed in~\cite{Russel:2018pqv} within the context of gravitational wave analysis for error estimation of the logarithm of the evidence from the thermodynamic integration method. We have not investigated this technique thoroughly enough to compare its performance with our own method.

\subsection{Temperature Placement Bias}
The placement of inverse-temperatures $\beta$ also affects the results of the numerical integration for the evidence~\citep{lartillot2006computing, xie2010improving}. Research into the proper placement of $\beta$ is ongoing in the field of Statistics~\citep{calderhead2009estimating, annis2019thermodynamic}. The studies of~\cite{friel2008marginal, xie2010improving, Russel:2018pqv} have used geometric placements of $\beta$ or drawn $\beta$ from a power-law distribution as plug-in temperature placement estimates. This is often a good place to start when choosing temperatures before conducting an MCMC analysis. The suggestion presented in~\cite{liu2016evaluating, de2013comparison, annis2019thermodynamic} is to conduct a pilot MCMC analysis where inverse temperatures are placed according to one of these default temperature placement rules. Then a followup re-analysis is conducted where additional inverse-temperatures are placed where the thermodynamic integrand changes rapidly or the behavior of the first derivative of the thermodynamic integrand is not smooth or well-behaved. Additional re-analyses can be conducted if the evidence integration method does not seem stable or is not measured at a precision adequate for the analysis. It is recommended in~\cite{annis2019thermodynamic} that more than $40$ temperatures be used, and we have used $>50$ inverse-temperatures within the context of gravitational wave data analysis. In our studies we have relied primarily on visual inspection and the suggestions in this paragraph for temperature placement.

A potential improvement for temperature placement that improves upon visual inspection of the thermodynamic integrand is presented in~\cite{friel2014improving}. The method of ~\cite{friel2014improving} calculates the intersection of the linear slopes of the derivatives of the thermodynamic integrand from two adjacent $\beta$. We defer to~\cite{friel2014improving} for additional details on stopping rules for $\beta$ placement. Finally, an additional possible improvement is to consider the placement of $\beta$ from as a Bayesian inference problem. This Bayesian inference approach to numerical integration is known as Bayesian quadrature~\cite{diaconis1988bayesian}, and it has been specifically applied to the problem of thermodynamic integration in~\cite{briol2015probabilistic}.

\section{The Steppingstone Method for Estimating the Bayesian Evidence}
We can also use another Bayesian evidence estimation technique that makes use of multi-tempering MCMC analyses. The steppingstone method is very similar in to thermodynamic integration in that it requires multiple inverse-temperatures between $0$ and $1$ to calculate. The steppingstone method uses importance sampling between adjacent temperatures to estimate the contribution to the marginal likelihood $\mathcal{Z}$ at each interval $\beta_{i-1}$-$\beta_i$. Before we present the derivation of the steppingstone method we provide a brief, but useful derivation of another identity, called the harmonic mean estimator for the evidence. For the following section we suppress use of $\vec{\theta}$ and $\mathbf{d}$ in our notation.

For the derivation of the harmonic mean estimator we follow a simplified version of the derivation presented in \citep{newton1994approximate}. From the definition of the evidence we can write:
\begin{equation}
    \frac{1}{\mathcal{Z}} = \frac{1}{\int \pi \, \mathcal{L} \, d\theta}.
\end{equation}
Here we can substitute the numerator with $\int \pi d\theta = 1$, which gives:
\begin{equation}
    \frac{1}{\mathcal{Z}} = \frac{\int \pi \, d\theta}{\int \pi \, \mathcal{L} \, d\theta}.
\end{equation}
Now we multiply both the numerator and denominator by $\mathcal{P}/\mathcal{P}$ to get:
\begin{equation}
    \frac{1}{\mathcal{Z}} = \frac{\int \frac{\pi}{\mathcal{P}} \mathcal{P} \, d\theta}{\int \frac{\pi \, \mathcal{L}}{\mathcal{P}}\mathcal{P} \, d\theta}.
\end{equation}
Which we simplify using Bayes' theorem to substitute out for $1/\mathcal{P}$ to give:
\begin{equation}
    \frac{1}{\mathcal{Z}} = \frac{\int \frac{\pi \mathcal{Z}}{\pi \mathcal{L}} \mathcal{P} \, d\theta}{\int \frac{\pi \, \mathcal{L} \mathcal{Z}}{\pi \mathcal{L}}\mathcal{P} \, d\theta}.
\end{equation}
Cancelling out terms of $\pi$ and moving terms of $\mathcal{Z}$ out of the integral to cancel, this gives:
\begin{equation}
    \frac{1}{\mathcal{Z}} = \frac{\int \frac{1}{\mathcal{L}} \mathcal{P} \, d\theta}{\int \mathcal{P} \, d\theta} = \int \frac{1}{\mathcal{L}} \mathcal{P} \, d\theta.
\end{equation}
Therefore we can express the inverse of the evidence as:
\begin{equation}\label{eqn:harm_mean}
    \frac{1}{\mathcal{Z}} = \langle \mathcal{L}^{-1} \rangle_{\mathcal{P}}.
\end{equation}
In Eq.~\ref{eqn:harm_mean} we have the inverse of the evidence is equal to the average value of the inverse of the likelihood when sampled from the posterior distribution. The harmonic mean estimator of the evidence is typically poorly behaved in the context of MCMC but it is a useful identity~\cite{xie2010improving}.

We follow~\cite{annis2019thermodynamic} in the derivation of the steppingstone estimator. Recall from Eq.~\ref{eq:thermoint} that the marginal likelihood can be expressed as:
\begin{equation}
    \mathrm{ln} \, \mathcal{Z} = \mathrm{ln} \, \mathcal{Z}_{\beta=1} - \mathrm{ln} \, \mathcal{Z}_{\beta=0},
\end{equation}
which is equivalent to:
\begin{equation}
    \mathcal{Z} = \frac{\mathcal{Z}_{\beta=1}}{\mathcal{Z}_{\beta=0}}.
\end{equation}
Without loss of generality we can consider a set of $100$ inverse-temperatures $\beta$ uniformly distributed between $0$ and $1$ such that
\begin{equation}
    \mathcal{Z} = \frac{\mathcal{Z}_{\beta = 0.01}} {\mathcal{Z}_{\beta = 0}}  
    \times \frac{\mathcal{Z}_{\beta = 0.02}} {\mathcal{Z}_{\beta = 0.01}}
    \times \ldots \times \frac{\mathcal{Z}_{\beta = 0.99}} {\mathcal{Z}_{\beta = 0.98}} \times \frac{\mathcal{Z}_{\beta = 1}} {\mathcal{Z}_{\beta = 0.99}}.
\end{equation}
This generalizes to
\begin{equation}\label{eqn:ssa_prod_series}
     \mathcal{Z} = \prod_{i=1}^{N_\beta} \frac{\mathcal{Z}_{\beta_{i}}}{\mathcal{Z}_{\beta_{i-1}}}.
\end{equation}
Here we use the ordering on $\beta$, as $\beta_0=0 < \beta_1 < ... < \beta_{N_\beta -1} < \beta_{N_\beta} = 1$. Finally, then, consider the evidence for the  power-posterior at inverse-temperature $\beta_i$ given as:
\begin{equation}
    \mathcal{Z}_{\beta_i} = \int \pi \mathcal{L}^{\beta_i} \, d\theta.
\end{equation}
We now divide by $1$ via $\int \pi \, d\theta$ and multiply by $1$ via $\mathcal{P}_{\beta_{i-1}} \, / \, \mathcal{P}_{\beta_{i-1}}$ in the numerator and denominator to get:
\begin{equation}
    \mathcal{Z}_{\beta_i} = \left(\int \frac{\pi \mathcal{L}^{\beta_i}}{\mathcal{P}_{\beta_{i-1}}} \mathcal{P}_{\beta_{i-1}} \, d\theta \right )\bigg / \left( \int \frac{\pi}{\mathcal{P}_{\beta_{i-1}}} \mathcal{P}_{\beta_{i-1}} \, d\theta \right).
\end{equation}
Using Bayes' theorem we substitute $\mathcal{P}_{\beta_{i-1}}$ = $(1/\mathcal{Z}_{\beta_{i-1}}) \, \pi \, \mathcal{L}^{\beta_{i-1}}$ to get:
\begin{equation}
    \mathcal{Z}_{\beta_i} = \left (\int \frac{\pi \mathcal{L}^{\beta_i} \mathcal{Z}_{\beta_{i-1}}}{\pi \mathcal{L}^{\beta_{i-1}}} \mathcal{P}_{\beta_{i-1}} \, d\theta \right) \bigg / \left(\int \frac{\pi \mathcal{Z}_{\beta_{i-1}}}{\pi \mathcal{L}^{\beta_{i-1}}} \mathcal{P}_{\beta_{i-1}} \, d\theta\right).
\end{equation}
Terms of $\mathcal{Z}_{\beta_{i-1}}$ are independent of $\theta$ and so can be moved out of the integral where they cancel. We can also cancel terms of $\pi$ to get
\begin{equation}
    \mathcal{Z}_{\beta_i} = \left(\int \frac{ \mathcal{L}^{\beta_i} }{\mathcal{L}^{\beta_{i-1}}} \mathcal{P}_{\beta_{i-1}} \, d\theta\right) \bigg / \left(\int \frac{1}{ \mathcal{L}^{\beta_{i-1}}} \mathcal{P}_{\beta_{i-1}} \, d\theta\right).
\end{equation}
Finally we recognize that in the denominator we have Eq.~\ref{eqn:harm_mean} for the inverse of the evidence at the inverse-temperature $\beta_{i-1}$. With some additional simplifications in the numerator we can have
\begin{equation}
    \mathcal{Z}_{\beta_i} = \mathcal{Z}_{\beta_{i-1}} \int \mathcal{L}^{\beta_i - \beta_{i-1}}  \mathcal{P}_{\beta_{i-1}} \, d\theta.
\end{equation}
Thus we arrive at the key ingredient for the steppingstone estimator:
\begin{equation}\label{eqn:ssa_identity}
    \frac{\mathcal{Z}_{\beta_i}}{\mathcal{Z}_{\beta_{i-1}}} = \int \mathcal{L}^{\beta_i - \beta_{i-1}}  \mathcal{P}_{\beta_{i-1}} \, d\theta = \langle \mathcal{L}^{\beta_i - \beta_{i-1}} \rangle_{\mathcal{P}_{\beta_{i-1}}}.
\end{equation}
We suppress some of the notation in Eq.~(\ref{eqn:ssa_identity}) such that $\langle \mathcal{L}^{\beta_i - \beta_{i-1}} \rangle_{\mathcal{P}_{\beta_{i-1}}} \equiv \langle \mathcal{L}^{\beta_i - \beta_{i-1}} \rangle_{\beta_{i-1}}$ and combine Eq.~\ref{eqn:ssa_identity} into Eq.~\ref{eqn:ssa_prod_series} to  give the steppingstone estimator for the evidence:
\begin{equation}\label{eqn:steppingstone}
    \mathcal{Z} = \prod_{i=1}^{N_\beta} \langle \mathcal{L}^{\beta_i - \beta_{i-1}} \rangle_{\beta_{i-1}}. 
\end{equation}
Some care needs to be taken in the implementation of Eq.~(\ref{eqn:steppingstone}) as the form presented is not numerically stable and we often must use the log likelihood and log evidence in place of the likelihood and the evidence. A numerically stable form of the logarithm of Eq.~\ref{eqn:steppingstone} is presented in~\cite{xie2010improving}. It is noted by ~\cite{xie2010improving} that the logarithm of the evidence in the steppingstone estimator exhibits some level of bias as an estimator of the marginal likelihood. This bias is small when many inverse-temperatures are used, and it was shown in~\cite{xie2010improving} that the steppingstone estimator in many cases outperforms the trapezoidal rule for thermodynamic integration with the same inverse-temperatures.

\subsection{Monte Carlo Error}
In~\cite{xie2010improving} there is also an expression for the estimated variance of the logarithmic steppingstone estimator using an approximation method called the $\delta$ method~\citep{oehlert1992note}. The expression in~\cite{xie2010improving} for the variance of the logarithm of the evidence is however not presented in a numerically stable version. We use a numerically stabilized version of the variance estimator in our gravitational wave analysis through a module that we have implemented in \pycbc{}\. We have found the variance estimate from the $\delta$ method is typically comparable to the Monte Carlo error estimate in the thermodynamic integration method. Repeated runs where the random seed for the MCMC analysis was changed has indicated that the variance estimate from presented in~\cite{xie2010improving} is a plausible confidence interval estimate for the Monte Carlo error. 

\subsection{Convergence Error}
The method for calculating the error on the steppingstone estimator due to convergence error is algorithmically identical to the thermodynamic integration method presented in Section~\ref{sec:evidence_convergence}. The convergence of the evidence estimation for the steppingstone estimator for a gravitational wave analysis can be seen in Fig.~\ref{fig:integral_convergence}.

\subsection{Temperature Placement Bias}
At the current time optimal placement of inverse-temperatures $\beta$ remains an active area of research~\citep{xie2010improving,annis2019thermodynamic}. For a large number of inverse-temperatures $\beta$ the study of~\cite{xie2010improving} showed that the steppingstone estimator converges to the correct evidence estimate faster than the thermodynamic integration method and~\cite{Russel:2018pqv} showed that this is also true in applications to gravitational wave astronomy. Both~\cite{xie2010improving} and~\cite{Russel:2018pqv} indicate that the evidence estimation of the steppingstone estimator may be less sensitive to temperature placement than the thermodynamic integration method implemented via the trapezoidal rule.

\section{The Savage-Dickey Density Ratio Method}\label{sec:sddr_derivation}
We also consider another MCMC method for estimating Bayes factors. This method is called the Savage-Dickey density ratio method and it requires two models, wherein one hypothesis is nested in the other parameter space of the other hypothesis. This requirement that the two models be nested is very restrictive, but it has enjoyed considerable use in the field of Cosmology~\cite{hobson2010bayesian}. We derive the method following~\cite{wagenmakers2010bayesian}. We can consider two hypotheses, $\mathrm{H}_{\mathrm{simple}}$ and $\mathrm{H}_{\mathrm{complex}}$. The hypothesis $\mathrm{H}_{\mathrm{simple}}$ is nested within $\mathrm{H}_{\mathrm{complex}}$, which is to say that when a critical parameter in $\mathrm{H}_{\mathrm{complex}}$, $A$, tends towards a critical value, in this case we use $0$, then $\mathrm{H}_{\mathrm{complex}}$ becomes identical to $\mathrm{H}_{\mathrm{simple}}$. We can formalize this by stating the prior distributions from these hypotheses in the following way:
\begin{eqnarray}
    \pi \left(\vec{\theta}_{\mathrm{simple}} \, | \, \mathrm{H}_{\mathrm{simple}}\right)  &\equiv& \pi \left(\left \alpha, \beta, \gamma, \ldots  \right \} | \mathrm{H}_{\mathrm{simple}} \right)  \\
    \pi \left(\vec{\theta}_{\mathrm{complex}} \, | \, \mathrm{H}_{\mathrm{complex}}\right) &\equiv& \pi \left(\left \alpha, \beta, \gamma \ldots , A \right \}| \mathrm{H}_{\mathrm{complex}}\right).
\end{eqnarray}
Here $\pi \left(\vec{\theta}_{\mathrm{simple}} \, | \, \mathrm{H}_{\mathrm{simple}}\right)$ represents the prior distribtuion on parameters in the simple hypothesis, while $\pi \left(\vec{\theta}_{\mathrm{complex}} \, | \, \mathrm{H}_{\mathrm{complex}}\right)$ represents the prior distribution on parameters in the complex hypothesis. The parameters $\alpha, \beta, \gamma$ are nuisance parameters in the context of this Savage-Dickey density ratio method, and are shared parameters between the simple and complex hypotheses. We abbreviate the notation by writing the prior under the simple hypothesis  as $\pi_{\mathrm{complex}} \left(\psi \right)$ and the prior under the more complex hypothesis as $\pi_{\mathrm{complex}} \left(\psi, A\right)$. Here the dependence on hypotheses is denoted by the subscript simple or complex, and $\psi$ denotes all parameters that are not $A$. In order for the Savage-Dickey Density Ratio method to hold true the following expression must be satisfied:
\begin{equation}\label{eqn:sddr_condition}
    \lim_{A \to 0} \pi_{\mathrm{complex}} \left(\psi | A\right) = \pi_{\mathrm{simple}}\left(\psi\right).
\end{equation}
This states that in the limit that $A$ tends to $0$, the prior parameter space in the complex hypothesis becomes identical to the prior parameter space in the simple hypothesis. Under these conditions we then consider the definition of the Bayes factor:
\begin{equation}
    \mathcal{B}^{\mathrm{complex}}_{\mathrm{simple} \equiv \frac{\mathcal{Z}_{\mathrm{complex}}(\mathbf{d})}{\mathcal{Z}_{\mathrm{simple}}(\mathbf{d})}.
\end{equation}
The denominator can be expressed as
\begin{equation}\label{eqn:evidence_sub_sddr}
    \mathcal{Z}_{\mathrm{simple}}(\mathbf{d}) = \int \pi_{\mathrm{simple}}\left(\psi\right) \, \mathcal{L}_{\mathrm{simple}} \left(\mathbf{d} | \psi \right)  d\psi.
\end{equation}
Since the models are nested, the prior and the likelihood under the complex hypothesis at $A=0$ is equivalent to the prior and likelihood under the simple hypothesis. This gives
\begin{equation}\label{eqn:sddr_sub_eqs1}
\pi_{\mathrm{complex}}\left(\psi, A=0\right) = \pi_{\mathrm{simple}}\left(\psi \right) \end{equation}
and
\begin{equation}
\label{eqn:sddr_sub_eqs2}
\mathcal{L}_{\mathrm{complex}}\left(\mathbf{d}|\psi, A=0\right) = \mathcal{L}_{\mathrm{simple}}\left( \mathbf{d} | \psi \right).
\end{equation}
If we substitute Eqs.~\ref{eqn:sddr_sub_eqs1} and \ref{eqn:sddr_sub_eqs2} into Eq.~\ref{eqn:evidence_sub_sddr} we get:
\begin{equation}
    \mathcal{Z}_{\mathrm{simple}}(\mathbf{d}) = \int \pi_{\mathrm{complex}}\left(\psi, A=0\right) \, \mathcal{L}_{\mathrm{complex}} \left(\mathbf{d} | \psi, A=0 \right)  d\psi.
\end{equation}
Integrating this over all $\psi$, leaves the $A=0$ unintegrated over leaving us with $\mathcal{Z}_{\mathrm{simple}} = \mathcal{L}_{\mathrm{complex}} \left(\mathbf{d} | A=0 \right)$. Using Bayes theorem, we can rewrite $\mathcal{L}_{\mathrm{complex}} \left(\mathbf{d} | A=0 \right) = [\mathcal{P}_{\mathrm{complex}}(A=0 | \mathbf{d}) \, \mathcal{Z}_{\mathrm{complex}}(\mathbf{d})] / \pi_{\mathrm{complex}} (A=0)$. This leaves us with:
\begin{equation}
    \mathcal{Z}_{\mathrm{simple}}\left(\mathbf{d}\right) = \frac{\mathcal{P}_{\mathbf{complex}}\left(A=0 | \mathbf{d}\right) \mathcal{Z}_{\mathrm{complex}} \left(\mathbf{d} \right)} {\pi_{\mathrm{complex}} \left(A=0\right)}.
\end{equation}
Finally, then we arrive at the Savage Dickey density ratio Bayes factor
\begin{equation}\label{sddr_bayes_factor}
    \mathcal{B}^{\mathrm{complex}}_{\mathrm{simple}} = \frac{\pi_{\mathrm{complex}}\left(A=0\right)}{\mathcal{P}_{\mathrm{complex}}\left(A=0 | \mathbf{d}\right)}.
\end{equation}
Here the Bayes factor for the complex hypothesis relative to the simple hypothesis is the ratio of the prior density at the critical value $A$=$0$ to the posterior density at the critical value $A$=$0$ when sampling from the complex hypothesis' parameter space. This all assumes that the simple hypothesis is nested within the complex hypothesis. A more thorough and rigorous derivation is presented in ~\cite{verdinelli1995computing} where an additional term is multiplied to Eq.~(\ref{eq:sddr_bayes_factor}). For our purposes we will not need to use this correction term.

The Savage-Dickey density ratio method requires the estimation of probability densities at a point in the marginal prior and posterior distributions for the nesting parameter $A$. We explore four methods for accurate probability density estimation. The first two methods are simple histogram approaches, the third estimate is a Gaussian kernel density estimator, and the last method is a cubic spline density estimator.

\subsection{Histogram Methods}
An simple method for estimating the probability density function is to histogram the samples by counts and then normalize the histogram to integrate to unity. Under this methodology the only relevant parameter to fitting the histogram to the data is in the choice of bin-width, sometimes called bandwidth.

We use two plug-in bin-width algorithms to fit a histogram to the data. The first method is Scott's rule~\citep{scott1979optimal} . Scott's rule is optimal if the underlying density of the function is normally distributed. The bin-width $h$ for this rule is defined as:
\begin{equation}
    h = \frac{3.5 \, \sigma}{N^{1/3}}.
\end{equation}
Here $\sigma$ represents the sample standard deviation of the data, and $N$ represents the number of samples in the data.

The second binning method is the Freedman-Diaconis binning method~\cite{Freedman1981}. The binning method makes use of the interquartile range (IQR) of the data rather than the standard deviation $sigma$. The IQR is defined as the difference between the $75^{\mathrm{th}}$ and $25^{\mathrm{th}}$ percentiles of the data. The bin-width $h$ in the Freedman-Diaconis is
\begin{equation}
    h = \frac{2 \, \mathrm{IQR}}{N^{1/3}.}
\end{equation}
Where $N$ represents the number of samples in the data.

There are other plug-in bin-width algorithms available but we have found that these two methods are very robust towards estimating the density functions of some common and expected probability distribution functions.

\subsection{A Gaussian Kernel Density Estimator}
We use a Gaussian kernel density estimator available in the Python package GetDist~\citep{lewis2015getdist}. GetDist is intended to accurately estimate the underlying one-dimensional and two-dimensional posterior probability distribution functions from a Bayesian MCMC analysis. A Gaussian kernel density estimator uses small truncated-Gaussian distributions centered at the samples of the data rather than individual bin-counts such as in a histogram. The Gaussian kernel density estimator then combines the sum of the these truncated Gaussian distributions to create a smooth probability distribution function.

The advantage that GetDist offers over other Gaussian kernel density estimators is that it comes with a robust linear-boundary bias correction to the standard Gaussian kernel density estimator. Sharp boundaries on the distribution function can cause bias to the probability distribution function estimation for Gaussian kernel density estimators~\citep{lewis2015getdist}. The Savage-Dickey density ratio method often requires us to estimate the density of the posterior distribution at the boundary of the distribution and so an estimator that can give an unbiased estimate of this density is highly desirable. There are additional features and bandwidth optimization algorithms present in GetDist for accurate density estimation but we defer to~\cite{lewis2015getdist} for the full details.

\subsection{The Logspline Spline Estimator}
The logspline density estimator of~\cite{stone1997polynomial} is written as a software package in R. The logspline software package estimates the probability density function of a set of data through a univariate cubic spline fit to the log-probability density. The package places knots of a cubic spline along the axis of the data through an internally defined likelihood function and data censoring module. The package uses Bayesian model-selection based on the Akaike Information Criterion (AIC)~\citep{akaike1981likelihood} and the Bayesian Information Criterion (BIC)~\citep{schwarz1978estimating} to ensure goodness of fit and to avoid overfitting to the data. The AIC is a model selection routine founded in information theory, while the BIC is based on an approximation of the Bayes factor. The full details of the logspline density estimator are beyond the scope of this dissertation. We utilize the maximum likelihood (best) fit to the probability distribution function from the packages' model selection routine. The logspline density estimator of~\cite{stone1997polynomial} is considered one of the most accurate one-dimensional probability distribution function estimators in the R language~\cite{deng2011density} and the logspline package comes specifically recommended in~\cite{wagenmakers2010bayesian} for the Savage-Dickey density ratio test.

\subsection{Error Analysis of Savage Dickey Density Ratio}
There are a variety of potentially different approaches to estimate the uncertainty in the Savage-Dickey density ratio method.
The most straightforward method is to conduct multiple MCMC analyses using the same prior distribution to get multiple
statistically independent estimates of the posterior distribution. The histogram methods using Scott's binning rule, the
Freedman-Diaconis binning rule, the GetDist Gaussian kernel density estimator, and the logspline density estimator can then
be checked for each posterior distribution. A mean value and variance estimate from these re-analyses could then indicate
the variability of the estimators. Bayesian MCMC analyses, especially in the context of gravitational wave data analysis,
can be computationally expensive and so we take a different approach.

A simple method for a quick sketch of a confidence interval on a test statistic can be achieved through a bootstrap method presented in~\cite{efron1992bootstrap}. The bootstrap method resamples the data with replacement providing a new distribution of data to estimate the test statistic on. This can be done many times to see how variable the test statistic is on a dataset that is drawn from the same population. A confidence interval can be estimated for these point-estimates of the Savage-Dickey density ratio by resampling the posterior distribution with replacement and recalculating the Savage-Dickey density ratio. After $N$ iterations a $90 \%$ confidence interval can be drawn. The bootstrap method is a form of cross-validation. There are many different kinds of cross-validation techniques available with potential to improve our confidence interval estimation~\cite{efron1997improvements, vehtari2017practical}, but we do not explore them here.

Now that we have established all of the tools that we need for Bayesian hypothesis testing we discuss their application in gravitational wave analysis.

\section{Prior Distributions for Gravitational Wave Analysis}
Choosing a set of prior distributions for our Bayesian inference requires
us to make choices on acceptable distributions of values for parameters that
characterize the physics of gravitational waves from compact binary
coalescence that we wish to model. Here we briefly describe some considerations when choosing
prior distributions and the necessary parameters for Bayesian inference
for gravitational waves from compact binary coalescences.

There are a number of parameters that describe the gravitational waves
radiated from a compact binary coalescence. The compact binaries'
gravitational wave is parametrized by the 
the component masses $m_{1,2}$ of the binary as well as the three-dimensional spin vectors
$\bf{s}_{1,2}$ of each of the compact objects~\cite{Hawking:1987en}.
The waveform observed by the
Earth-based detector network depends on six additional parameters: the
signal's time of arrival $t_\mathrm{coalescence}$, the binary's luminosity distance
$d_\mathrm{L}$, and four Euler angles that describe the transformation from the
binary's radiation frame to the detector network frame~\cite{Wahlquist:1987rx}.  These
angles are typically written as the binary's right ascension $\alpha$,
declination $\delta$, a polarization angle $\Psi$, and the inclination angle
$\iota$. The inclination angle is the angle between the binary's angular momentum axis and the line of
sight. The convention adopted in  for $\iota$ denotes $\iota = 0$ as a
``face-on'' binary (line of sight parallel to binary angular momentum), $\iota
= \frac{\pi}{2}$ as an ``edge-on'' binary (line of sight perpendicular to
binary angular momentum), and $\iota = \pi$ as a ``face-off'' binary (line of
sight anti-parallel to binary angular momentum).  

Binary mergers present a challenging problem for Bayesian inference, because the
dimensionality of the signal parameter space is large. This is further
complicated by degeneracies between many of the signal's parameters. For example, to
leading order the gravitational waveform depends on the chirp mass
$\mathcal{M} \ equiv (m_1 m_2)^{1/5} / (m_1 + m_2)^{3/5}$~\cite{PhysRev.131.435}.
The mass ratio $q \equiv m_1 / m_2$ enters the waveform at
higher orders and is more difficult to measure. This results in an
amplitude-dependent degeneracy between the component
masses~\cite{Christensen:2001cr}. Similarly, the binary's mass ratio can be
degenerate with its spin~\cite{Hannam:2013uu}. To see this we recall from
Chapter 2 of this dissertation that
the effective spin of a binary along the axis of angular momentum is
$\chi_{\text{eff}} = (c/G) (\mathbf{S_1}/m_1 + \mathbf{S_2}/m_2)
\cdot (\mathbf{\hat{L}}/(m_1 + m_2)$. Refactoring the binary component
masses in terms of the mass ratio $q$ we get:
\begin{equation}

\end{equation}

Given a set of parameters $\vec{\theta}$, one can obtain a model of the
gravitational-wave signal from a binary merger using a variety of different
methods, including: post-Newtonian theory (see e.g. Ref.~\cite{Blanchet2006}
and references therein), analytic models calibrated against numerical
simulations~\cite{Buonanno:1998gg,Buonanno:2000ef,Damour:2000we,Damour:2001tu,Ajith:2007qp,Ajith:2009bn,Santamaria:2010yb},
perturbation theory~\cite{Teukolsky:1972my,Berti:2009kk}, and full numerical
solution of the Einstein equations (see e.g. Ref.~\cite{Cardoso:2014uka} and
references therein). The specific choice of waveform model and marginal prior distribution probabilities
on parameters specify a $\pi(\vec{\theta} \, | \, \mathrm{H})$ for an MCMC analysis.
The decision of what parameters $\vec{\theta}$ and prior $\pi(\vec{\theta} \, | \, \mathrm{H})$ 
to use depends on the
physics that we wish to explore in the model. A variety of waveform models are available
for use in \pycbc{}\ Inference, either directly implemented in \pycbc{}\ or via 
the LIGO Algorithm Library (LAL)~\cite{lal}.

\section{The Likelihood function for Gravitational Wave Analysis}
The gravitational wave strain data observed by gravitational-wave detector networks
enters Bayes' theorem through the likelihood $\mathcal{L}(\mathbf{d} | \vec{\theta})$
 in Eq.~\ref{eqn:BayesTheorem_PE}.
Currently, \pycbc{}\ Inference assumes that each detector produces stationary,
Gaussian noise $n_{i}(t)$ that is uncorrelated between the detectors in the
network. The observed data is then $d_{i}(t) = n_{i}(t) + s_{i}(t)$, where
$s_i(t)$ is the gravitational waveform observed in the $i$-th detector.  For
detectors that are not identical and co-located (as in the case of the Advanced LIGO-Virgo network), each detector observes a slightly different waveform due to their different antennae patterns which are functions of the sky position (right ascension and declination) and polarization~\cite{Wahlquist:1987rx}.

Under these assumptions, the appropriate form of $\mathcal{L}(\mathbf{d} \, | \, \vec{\theta})$
is the likelihood for a signal of known morphology in Gaussian noise (see
e.g.  Ref.~\cite{Wainstein:1962} for its derivation), which is
given by
\begin{equation}\label{eqn:likelihood}
\mathcal{L}(\mathbf{d} \, | \, \vec{\theta}) \propto \exp \left[ -\frac{1}{2} \sum_{\limit i = 1}^{\limit N} \left \langle \tilde{n}_{i}(f) | \tilde{n}_{i}(f)\right\rangle \right] \\ 
      = \exp \left[ -\frac{1}{2} \sum_{\limit i = 1}^{\limit N} \left \langle \tilde{d}_{i}(f) - \tilde{s}_{i}(f, \vec{\theta})| \tilde{d}_{i}(f) - \tilde{s}_{i}(f, \vec{\theta})\right \rangle \right] ,
\end{equation}
where $N$ is the number of detectors in the network. The constant of proportionality for this Gaussian likelihood
is not important for Bayesian inference under MCMC simulation since the constant drops out
when proposals are made in the parameter space~\cite{cbiwer_thesis}.
This proportionality constant also cancels out when calculating Bayes factors.
The inner product $\langle\tilde{a} \, | \, \tilde{b}\rangle$ is
\begin{equation}
\left \langle \tilde{a}_i(f) \, | \, \tilde{b}_i(f)\right \rangle = 4 \mathcal{R} \int_{0}^{\infty} \frac{\tilde{a}_i(f) \tilde{b}_i(f)}{S^{(i)}_n(f)} \mathrm{d}f \,,
\end{equation}
where $\mathcal{R}$ denotes the real part of the integral,
$S^{(i)}_n(f)$ is the power spectral density of the of the $i$-th
detector's noise. Here, $\tilde{d}_{i}(f)$ and $\tilde{n}_{i}(f)$ are the
frequency-domain representations of the data and noise, obtained by a Fourier
transformation of $d_{i}(t)$ and $n_{i}(t)$, respectively. The model waveform
$\tilde{s}_{i}(f, \vec{\theta})$ may be computed directly in the frequency
domain, or in the time domain and then Fourier transformed to the frequency
domain. For a specified prior distribution on parameters,
\pycbc{}\ Inference calculates Eq.~\ref{eqn:likelihood} and
completes Bayesian inference for estimating posterior distributions and
estimating Bayesian evidences.

\newpage
\begin{figure}[th]
  \includegraphics[width=\textwidth]{figs/chapter5/log_odds_probability.png}
  \caption{(\textit{Light blue-solid}) The probability of hypothesis $\mathrm{H}_A$ being favored over hypothesis $\mathrm{H}_B$ after observation of the data $\mathbf{d}$ when considering calculating the natural log of the odds ratio for each hypothesis. (\textit{Red dashed-solid}) The posterior probability of $\mathrm{H}_B$ is the complement of the posterior probability of $\mathrm{H}_A$ if there are only two hypotheses available to test. When $\mathrm{log}_{10} \; \mathcal{O} = 0$, the probability for each hypothesis is $50\%$.}
  \label{fig:log_odds_v_probability}
\end{figure}

\begin{figure}[th]
  \includegraphics[width=\textwidth]{figs/chapter5/log_odds_z_score.png}
  \caption{The z-score pertaining to the same level of probability for  hypothesis 1 being favored over hypothesis 2 when considering the $\mathrm{ln}_{10} \; \mathcal{O}$. When $\mathrm{ln} \; \mathcal{O} = 0$, the z-score is $0 \sigma$ and the probability for each hypothesis is $50\%$. A z-score of $>5 \sigma$ has the same probability value as an odds ratio of $> 10^7$.}
  \label{fig:log_odds_v_z_score}
\end{figure}

\begin{figure}[th]
\centering
\includegraphics[width=1.0\textwidth]{figs/chapter6/gooseneck_plots_linear.png}
\caption{The subplots of the thermodynamic integrand and subsequent derivatives of the thermodynamic integral. (\textit{Top}) The thermodynamic integrand when compared to the inverse-temperature $\beta$. The curve should be smooth and montonic, however it is very difficult to inspect the integrand on a linear $\beta$ scale. (\textit{Middle}) The second derivative of the logarithm of the evidence is the variance of the power-posterior at an inverse temperature $\beta$. There is some indication that an inflection point happens in the curvature of the integrand at high temperature. (\textit{Bottom}) The third derivative of the logarithm of the evidence is also the third-order cumulant of the power-posterior distributions at an inverse-temperature $\beta$. It is difficult to inspect the behavior of this derivative on the linear $\beta$ scale.}
\label{fig:gooseneck_linear}
\end{figure}

\begin{figure}[th]
\centering
\includegraphics[width=1.0\textwidth]{figs/chapter6/gooseneck_plots_log.png}
\caption{The subplots of the thermodynamic integrand and subsequent derivatives of the thermodynamic integral. (\textit{Top}) The thermodynamic integrand when compared to the inverse-temperature $\beta$. The curve should be smooth and montonic, however there is some indication at $\beta = 10^{-9}$ that this condition is not strictly met in the Markov Chain Monte Carlo simulation. (\textit{Middle}) The second derivative of the logarithm of the evidence is the variance of the power-posterior at an inverse temperature $\beta$. This function should also be smooth however there is some indication that at high temperature that the derivatives are not stable. (\textit{Bottom}) The third derivative of the logarithm of the evidence is also the third-order cumulant of the power-posterior distributions at an inverse-temperature $\beta$. Here we can see that the derivatives are not very sable or smooth. This may motivate moving our analysis to new multi-tempered samplers that are optimized for thermodynamic integration.}
\label{fig:gooseneck_log}
\end{figure}

\begin{figure}[th]
\centering
\includegraphics[width=1.0\textwidth]{figs/chapter6/ti_monte_carlo_error.png}
\caption{The first subplot denotes the untempered log-likelihood samples when drawn from the power-posteriors at $\beta$. The expectation value of the untempered log-likelihood when drawn from these power-posteriors is the thermodynamic integrand and is plotted in red. The thermodynamic integral over all geometric paths given from the samples is drawn in the second subplot. The sample-log-integral distribution is approximately a Gaussian distribution. The standard error of the mean value of the log evidence is given by the sample standard deviation divided by the square root of the number of samples. The $90 \%$ confidence interval on the sample distribution in the log-evidence is drawn in dashed orange lines. The $90\%$ confidence region from this standard error is shaded in red. The final subplot is a zoom-in on this $90 \%$ confidence region showing the error estimate on the thermodynamic integral due to Monte Carlo sampling.}
\label{fig:ti_monte_carlo_error}
\end{figure}

\begin{figure}[th]
\centering
\includegraphics[width=1.0\columnwidth]{figs/chapter6/convergence_segmentation_lvc_sim.png}
\caption{The partitioning of the MCMC analysis to check on the convergence of the thermodynamic integrand and the thermodynamic integration. The dark-green bar at the top represents all of the samples collected by the MCMC analysis. This segment is divided into 12 segments represented by the light gray lines. The light-green segments represent chunks that independent samples can be drawn from. The dark region represents samples discarded as burn-in samples for the MCMC. The dark grey region represents data that is ahead of the chunk and thus not used in drawing independent samples for that chunk. Chunk $12$ produces the identical samples as drawing independent samples according to the $\mathrm{n_{acl}}$ algorithm from PyCBC at the end of the analysis.}
\label{fig:nacl_segments}
\end{figure}

\begin{figure}[th]
\centering
\includegraphics[width=1.0\columnwidth]{figs/chapter6/lsc_sim_integrand_progress.png}
\caption{The convergence of the thermodynamic integrand for a gravitational wave analysis using $51$ temperatures. This analysis neglected $\beta$ = $0$, but is otherwise an acceptable representation of the thermodynamic integrand. The Iteration-Start denotes the point is taken from a segment beginning with that MCMC iteration and ending with the MCMC iteration denoted as Iteration-End. These iterations correspond to the segments found in Fig.~\ref{fig:nacl_segments}. The logarithm of the evidence is shown also in the figure caption, and as the MCMC analysis progresses the integral converges to a set value. The thermodynamic integrand can be visually seen to converge to a S-like curve but the shape and curvature are unique to hypotheses and choice of data. Early in the MCMC analysis the thermodynamic integrand can be mishaped as the power-posteriors have not all converged. Experience has told us tha the power-posteriors that take the longest to converge tend to be in the region where the average log likelihood changes rapidly. Here this is in the region between $\beta$ $\in$ ($10^{-2} - 1$).}
\label{fig:integrand_convergence}
\end{figure}

\begin{figure}[th]
\centering
\includegraphics[width=1.0\columnwidth]{figs/chapter6/lvc_sim_evidence_convergence.png}
\caption{The convergence of the thermodynamic integral for a gravitational wave analysis using $51$ temperatures as a function of the MCMC iteration. These choice of points of iterations correspond to the segments found in Fig.~\ref{fig:nacl_segments}. As the analysis progresses the logarithm of the evidence from all quadrature methods tend towards a fixed value.}
\label{fig:integral_convergence}
\end{figure}

\newpage
\begin{sidewaystable}
\centering
\begin{tabularx}{1.0\textwidth}{c|c|c|c|c}
\hline\hline
Bayes factor & log Bayes factor & Posterior Probability & z-score & Interpretation \\
$\mathcal{B}^{\mathrm{H}_A}_{\mathrm{H}_B}$ & ln $\mathcal{B}^{\mathrm{H}_A}_{\mathrm{H}_B}$ & $\mathcal{P} \left(\mathrm{H}_A \, | \, \mathbf{d} \right)$ & $\sigma$ & - \\
\hline\hline
$2.7 \times 10^{-7}$ to $2.7 \times 10^{-5}$ & $-15.1$ to $-10.5$ & $2.7 \times 10^{-7}$ to $2.7 \times 10^{-5}$ & $-5$ to $-4$ & Very strong evidence for $\mathrm{H}_{B}$\\
$2.7 \times 10^{-5}$ to $0.0014$ & $-10.5$ to $-6.6$ & $2.7 \times 10^{-5}$ to $0.0014$ & $-4$ to $-3$ & Very strong evidence for $\mathrm{H}_B$ \\
$0.001$ to $0.022$ & $-6.6$ to $-3.8$ & $0.0014$ to $0.02$ & $-3$ to $-2$ & Strong evidence for $\mathrm{H}_B$ \\
$0.022$ to $0.18$ & $-3.8$ to $-1.7$ & $0.02$ to $0.15$ & $-2$ to $-1$ & Moderate evidence for $\mathrm{H}_B$ \\
$0.18$ to $1.0$ & $-1.7$ to $0$ & $0.15$ to $0.5$ & $-1$ to $0$ & Weak evidence for $\mathrm{H}_B$ \\
$1.0$ to $5.5$ & $0$ to $1.7$ & $0.5$ to $0.85$ & $\, 0$ to $\, 1$ & Weak evidence for $\mathrm{H}_A$ \\
$5.5$ to $45$ & $1.7$ to $3.8$ & $0.85$ to $0.98$ &  $\, 1$ to $\, 2$ & Moderate evidence for $\mathrm{H}_A$ \\
$45$ to $740$ & $3.8$ to $6.6$ & $0.98$ to $0.9986$ & $\, 2$ to $\, 3$ & Strong evidence for $\mathrm{H}_A$ \\
$740$ to $3.6 \times 10^{4}$ & $6.6$ to $10.5$ & $0.9986$ to $0.999972$ &  $\, 3$ to $\, 4$ & Very strong evidence for $\mathrm{H}_A$ \\
$3.6 \times 10^{4}$ to $3.6 \times 10^{6} $ & $10.5$ to $15.1$ & $0.999972$ to $0.99999972$ &  $\, 4$ to $\, 5$ & Very strong evidence for $\mathrm{H}_A$ \\
\hline\hline
\end{tabularx}
\caption{An empirical scale for evaluating the relative strength of evidence between hypothesis $\mathrm{H}_A$ and an alternative hypothesis $\mathrm{H}_B$ loosely adapted from Tables $1$ $\&$ $2$ of~\cite{trotta2008bayes}. We assume a prior odds ratio of unity between the two hypotheses, indicating no prior preference for either hypothesis. This makes the Bayes factor identical to the odds ratio and so we neglect the odds ratio here. Note that we have rounded to $2$ significant digits leaving a minor rounding discrepancy with Table~\ref{tab:results}. The interpretation column is only a rule of thumb and interpretations vary across scientific fields and experimental contexts.}\label{table:bayes_factor_interpretation}
\end{sidewaystable}


\newpage

\begin{sidewaystable}
\centering
\begin{tabularx}{1.0\textwidth}{c |c | c | c}
\hline\hline
Derivative & Result & Moment Representation & Cumulant \\
\hline\hline
\multirow{2}{*}{$\frac{d (\mathrm{ln} \, \mathcal{Z})}{d\beta}$} & \multirow{2}{*}{$\frac{\mathcal{Z}'}{\mathcal{Z}}$} & \multirow{2}{*}{$\langle \mathrm{ln} \, \mathcal{L} \rangle_\beta$} & \multirow{2}{*}{$\mu_1$} \\
 & & & \\
\hline
\multirow{2}{*}{$\frac{d^2 (\mathrm{ln} \, \mathcal{Z})}{d\beta ^2}$} & \multirow{2}{*}{$\frac{\mathcal{Z}''}{\mathcal{Z}} - \frac{\mathcal{Z}'^2}{\mathcal{Z}^2}$} & \multirow{2}{*}{$\langle (\mathrm{ln} \, \mathcal{L})^2 \rangle_\beta - \langle \mathrm{ln} \, \mathcal{L} \rangle^2_\beta$}  & \multirow{2}{*}{$\mu_2$ - $\mu_1^2$} \\
& & & \\
\hline
\multirow{2}{*}{$\frac{d^3 (\mathrm{ln} \, \mathcal{Z})}{d\beta ^3}$}  & \multirow{2}{*}{$\frac{\mathcal{Z}^{(3)}}{\mathcal{Z})}+ 2 \frac{\mathcal{Z}'^3}{\mathcal{Z}^3} -  3 \frac{\mathcal{Z}' \mathcal{Z}''}{\mathcal{Z}^2}$} &  \multirow{2}{*}{$\langle (\mathrm{ln} \, \mathcal{L})^3 \rangle_\beta  + 2 \langle \mathrm{ln} \, \mathcal{L} \rangle^3 - 3 \langle (\mathrm{ln} \, \mathcal{L})^2 \rangle_\beta \langle \mathrm{ln} \, \mathcal{L} \rangle_\beta$} & \multirow{2}{*}{$\mu_3 + 2\mu_1^3 - 3\mu_2 \mu_1$}  \\
& & & \\
\hline
\multirow{5}{*}{$\frac{d^4 (\mathrm{ln} \, \mathcal{Z})}{d\beta ^4}$} & \multirow{5}{*}{$\frac{\mathcal{Z}^{(4)}}{\mathcal{Z}} - 3 \frac{\mathcal{Z}''^2}{\mathcal{Z}^2} - 6 \frac{\mathcal{Z}'^4 }{\mathcal{Z}^4} - 4 \frac{\mathcal{Z}^{(3)} \mathcal{Z}'}{\mathcal{Z}^2} + 12 \frac{\mathcal{Z}'^2 \mathcal{Z}''}{\mathcal{Z}^3}$} & \multirow{5}{*}{\begin{multlined} $\langle (\mathrm{ln} \, \mathcal{L})^4 \rangle_\beta - 3 \langle (\mathrm{ln} \, \mathcal{L})^2 \rangle^2_\beta - 6 \langle \mathrm{ln} \, \mathcal{L} \rangle_\beta^4$ \\ $- 4 \langle (\mathrm{ln} \, \mathcal{L})^3 \rangle_\beta \langle \mathrm{ln} \, \mathcal{L} \rangle_\beta + 12 \langle (\mathrm{ln} \, \mathcal{L} )^2 \rangle_\beta \langle \mathrm{ln} \, \mathcal{L} \rangle^2_\beta$ \end{multlined}} & \multirow{5}{*}{\begin{multlined}$\mu_4 - 3 \mu_2^2 - 6 \mu_1^4$ \\ - $4\mu_3\mu_1 + 12\mu_2\mu_1^2$\end{multlined}} \\
 & & & \\
 & & & \\
 & & & \\
 & & & \\
\hline\hline
\end{tabularx}
\caption{The table of derivatives of the thermodynamic integral along with the moment representation of the derivatives of the thermodynamic integral and a set of reference of cumulants~\cite{kardar2007statistical}. The cumulants are represented in terms of the non-central moments of a distribution so that the representation is made clear with the moment representation of the derivatives of the thermodynamic integral. The non-central moment $\mu_i$ represents the expectation value $\langle x^i\rangle$, where $x$ is the one dimensional independent variable of a probability distribution function~\cite{abramowitz1965handbook}. We only present four derivatives of the thermodynamic integral for both brevity and because our data are typically not accurate enough to estimate higher order cumulants. We have inspected that this relation holds up to the $7^{\mathrm{th}}$ derivative. Further orders could also be checked, although a formal proof may be more satisfactory.} \label{table:cumulants}
\end{sidewaystable}

