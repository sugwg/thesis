\section{Probability}
Here we simply outline a few simple rules for probability. The simplest rule of probability is that given a series of possible outcomes, the sum of the probabilities of the outcome must equal unity. This is expressed as:
\begin{equation}\label{eqn:probsumdiscrete}
    \sum_{i=1}^{N} p_i = 1.
\end{equation}
Here $p_i$ represents the probability mass function, or more simply, the probability of the \textit{$i^{th}$} outcome, given $N$ possible events. If the random variable is continuous then we simply express this as an integral:
\begin{equation}\label{eqn:probsumcontinuous}
    \int p \left( x \right) dx = 1.
\end{equation}
Where $p(x)$ represents the probability density function of a particular outcome $x$.

Finally, we describe the notion of conditional probability. The probability of two events occurring is $p(A \textrm{ and } B)$:
\begin{equation}\label{eqn:probAandprobB}
    p\left(A\textrm{ and }B\right) \equiv p(A,B) = p(A) \, p(B|A) = p(B) \, p(A|B)
\end{equation}
This new term here $p(B|A)$ is to be interpreted as the probability that event B occurs given that A has occurred, and similarly, $p(A|B)$ means the probability that event A occurs given that B has occurred.

\section{Bayes Theorem}
Expression from Eq. \ref{eqn:probAandprobB} motivates the theorem known as Bayes Theorem, which we will express as follows:
\begin{equation} \label{eqn:BayesTheorem_basic}
     p(H|D) = \frac{p(H) \, p(D|H)}{p(D)}.
\end{equation}
In this formulation we have written, the probability of the hypothesis given the data, $p(H|D)$, is sometimes called the posterior probability. The probability of the hypothesis being true is $p(H)$, and is often called the prior probability since it is what we believe prior to looking at the data. The probability of the data given the hypothesis, $p(D|H)$, which is called the likelihood. And finally we have the probability of obtaining the data, $p(D)$. We will devote a large amount of time in this work towards Bayes Theorem and its usefulness in conducting statistical inferences.

\section{Bayesian Hypothesis Testing}
\subsection{The Bayes Factor}
Another essential aspect of Bayesian inference is the evaluation of the statistical signficance of hypothesis choices. This occurs through evaluating the effectiveness of the choice in prior probability distribution. The marginal likelihood, $\mathcal{E}$, is the main driver behind establishing the level of evidence or support that the data has for a particular prior distribution choice. Simply put, the prior distribution that results in the largest evidence value is the model that has the most support.

Calculation of the odds for support of one hypothesis, $H_1$, over another hypothesis, $H_2$, is encapsulated in the following expression for the posterior odds ratio:
\begin{equation}\label{eqn:odds_ratio}
\mathcal{O}^{H_1\;\;}_{\;\;H_2} = \mathcal{B}^{H_1\;\;}_{\;\;H_2} \times \frac{\pi(H_1)}{\pi(H_2)}.
\end{equation}
In this equation, $\mathcal{O}^{H_1\;\;}_{\;\;H_2}$ represents the posterior odds that hypothesis $1$ is preferred over hypothesis $2$. The ratio of the evidences, $\mathcal{B}^{H_1\;\;}_{\;\;H_2} \equiv \frac{\mathcal{E}_{H_1}}{\mathcal{E}_{H_2}}$, between the two models is known as the Bayes factor. The Bayes factor provides an intuition for the relative support of one hypothesis over the other. The ratio $\frac{\pi(H_1)}{\pi(H_2)}$ represents our prior odds ratio, that is, how much more did we believe that hypothesis $1$ was preferred over hypothesis $2$ prior to our analysis. Said in another way, the prior odds ratio gives us a statement of what level of Bayes factor we would require before we begin to change our minds about the odds of hypothesis $2$ being better supported in the data than hypothesis $1$. When testing new physics, one may set the prior odds ratio to unity if one is fundamentally unsure about what hypotheses the data may support.

The posterior odds ratio then gives us a method for making a decision about whether to accept one hypothesis over the other hypothesis. One advantage to Bayesian hypothesis testing is that it gives us a straightforward method for testing hypotheses other than the null hypothesis that is commonly tested in Frequentist statistical inference. The downside however is that effectively and consistently computing Bayes factors remains an open area of research because of how difficult it can be to calculate the marginal likelihood. A conventional choice for hypothesis decision making is given to us by Jeffreys, and an alternative by Kass and Raftery 1995, see Fig. X.

An odds ratio can be converted into a probability of one hypothesis over another hypothesis through the following expression:
\begin{equation}\label{eqn:probability_odds_ratio}
    p^{H_1 \;\;}_{\;\;H_2} = \frac{\mathcal{O}^{H_1\;\;}_{\;\;H_2}}{1 + \mathcal{O}^{H_1\;\;}_{\;\;H_2}}.
\end{equation}
As such, a plot of the $\mathrm{log}_{10} \; \mathcal{O}^{H_1\;\;}_{\;\;H_2}$ can be made to suggest decision rules for odds ratios similar to choices on p-values in Frequentist statistics. As we can see in the plot below, when the odds ratio is 1 ($\mathrm{log}_{10} \; \mathcal{O} = 0$) the probability of one hypothesis versus another is $0.5$. 
\begin{figure}
  \includegraphics[width=\textwidth]{figs/chapter5/log10odds_probability.png}
  \caption{The probability of hypothesis 1 being favored over hypothesis 2 when considering the $\mathrm{log}_{10} \; \mathcal{O}$. When $\mathrm{log}_{10} \; \mathcal{O} = 0$, the probability for each hypothesis is $50\%$. At odds ratios close to 100 (0.01) the evidence becomes heavily stacked towards one hypothesis or another.}
  \label{fig:log10odds_v_probability}
\end{figure}
Furthermore, we can map this probability to a ranking statistic that is more familiar to Frequentists. That is the one-tailed z-score which states the integrated probability density from $-\infty$ to a particular multiple of the standard deviation of a Gaussian function. A z-score of $0 \sigma$ indicates a $50\%$ probability, while a z-score of $5 \sigma$ is $\sim$ $1-10^-7$ probability. We place a plot of this below for convenience.
\begin{figure}
  \includegraphics[width=\textwidth]{figs/chapter5/log10odds_z_score.png}
  \caption{The Frequentist z-score pertaining to the same level of probability for  hypothesis 1 being favored over hypothesis 2 when considering the $\mathrm{log}_{10} \; \mathcal{O}$. When $\mathrm{log}_{10} \; \mathcal{O} = 0$, the z-score is $0 \sigma$ and the probability for each hypothesis is $50\%$. A z-score of $>5 \sigma$ has the same probability value as an odds ratio of $> 10^7$.}
  \label{fig:log10odds_v_z_score}
\end{figure}

One convenient property of odds ratios is that we can stack evidence from multiple events if we continue to measure new data with our same prior hypotheses. In this manner, it is possible to take low significant results from multiple experiments and gradually build evidence for a hypothesis over many experiments.

\section{Practical matters concerning Bayes factor estimation}
We model the logarithm of the evidence from multi-tempered methods for each astrophysical hypothesis as a Gaussian distribution in log-likelihood, with mean $\mu_{\widehat{\mathrm{ln} \, \mathcal{Z}}}$ given by the particular multi-tempered method's point-estimate, and with standard deviation $\sigma_{\widehat{\mathrm{ln} \, \mathcal{Z}}}$ given from Eq.(~\ref{eq:errorprop}). Thus we treat the logarithm of the evidence as a random variable described by a Gaussian distribution as follows:
\begin{equation}\label{eqn:p_log_z}
    p(\widehat{\mathrm{ln} \, \mathcal{Z}}) = \left(\frac{1}{\sqrt{2 \pi \sigma_{\widehat{\mathrm{ln} \, \mathcal{Z}}}}} \right) \mathrm{exp} \left \{-\frac{\left(\mathrm{ln} \, \mathcal{Z} - \mu_{\widehat{\mathrm{ln} \, \mathcal{Z}}}\right)^2} {2 \sigma_{\widehat{\mathrm{ln} \, \mathcal{Z}}}}  \right\}.
\end{equation}
We can see the distributions of the logarithm of the evidence for the astrophysical hypothesis on $p$-$g$ mode instability for the unconstrained $\delta \phi$ prior in Fig.~\ref{fig:lvc_sim_log_evidence_distr}.

The logarithm of the Bayes factor is the difference of the logarithm of the evidence for one hypothesis and the logarithm of the evidence for another hypothesis:
\begin{equation}\label{eqn:log_bayes_factor}
    \mathrm{ln} \, \mathcal{B}^A_B = \mathrm{ln} \, \mathcal{Z}_{\mathrm{A}} - \mathrm{ln} \, \mathcal{Z}_{\mathrm{B}}
\end{equation}
However, since we treat $\mathrm{ln} \, \mathcal{Z}_{\mathrm{A}}$ as a random variable who's true value is unknown and so we must deal with the uncertainty in $\widehat{\mathrm{ln} \, \mathcal{Z}_{\mathrm{A}}}$. The logarithm of the Bayes factor then becomes the difference between two probability distribution functions. This can be solved via convolution and has been solved for the Gaussian case~\citep{bromiley2003products}. From \cite{bormily}, we can express $\widehat{\mathrm{ln} \, \mathcal{B}^A_B}$ as a Gaussian distribution function with mean $\mu_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B}} = \mu_{\widehat{\mathrm{ln} \, \mathcal{Z}_A}} - \mu_{\widehat{\mathrm{ln} \, \mathcal{Z}_B}}$ and standard deviation $\sigma_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B}} = \sqrt{\sigma_{\widehat{\mathrm{ln} \, \mathcal{Z}_A}}^2 + \sigma_{\widehat{\mathrm{ln} \, \mathcal{Z}_B}}^2 }$. This thus gives us the following expression for the distribution function that describes our uncertainty on the logarithm of the Bayes factor:
\begin{equation}\label{eqn:p_log_b}
    p(\widehat{\mathrm{ln} \, \mathcal{B}^A_B}) = \left(\frac{1}{\sqrt{2 \pi \sigma_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B}}}} \right) \mathrm{exp} \left \{-\frac{\left(\widehat{\mathrm{ln} \, \mathcal{B}^A_B} - \mu_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B})}\right)^2} {2 \sigma^2_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B}}}  \right\}.
\end{equation}

The expression in Eq.~(\ref{eqn:p_log_b}) is a Gaussian distribution function in $\widehat{\mathrm{ln} \, \mathcal{B}^A_B}$, but we often prefer to know the estimate on $\mathcal{B}^A_B$ and so we must transform coordinates. This transformation of coordinates, fortunately, is a well-known distribution called the log-normal distribution and it is able to be described in terms of the coordinates used in Eq.~(\ref{eqn:p_log_b}). Below we write out our log-normal probability distribution function for $\widehat{\mathcal{B}^A_B}$:
\begin{equation}
    p(\widehat{\mathcal{B}^A_B}) = \frac{1}{\widehat{\mathcal{B}^A_B} \, \sigma_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B}}} \frac{1}{2\pi} \mathrm{exp} \left \{-\frac{\left(\mathrm{ln} \, \widehat{\mathcal{B}^A_B} - \mu_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B})}\right)^2} {2 \sigma^2_{\widehat{\mathrm{ln} \, \mathcal{B}^A_B}}}  \right\}.
\end{equation}
This is the implementation that we have used to represent that Bayes factor in chapter BLAH. It is worth noting that for a sufficiently small standard deviation on the logarithm of the Bayes factor, the probability of the distribution function will look approximately Gaussian in shape. For the log-normal Bayes factor distribution the median value of the distribution is identical to the point-estimate Bayes factor, $\mathcal{B}^A_B = \mathrm{exp} \left[\mathrm{ln} \, \mathcal{Z}_{\mathrm{A}} - \mathrm{ln} \, \mathcal{Z}_{\mathrm{B}} \right]$. The expectation value (mean) of this log-normal distribution is always right of the median, while the mode of the distribution is left of the median. Large standard deviations on the logarithm of the evidence will create very long tails for the distribution of the Bayes factor, which makes decision-making based on Bayes factors more risky. Studies that use this estimation of the Bayes factor should consider limiting the error on the logarithm of the evidence to mitigate propagating large error to the Bayes factor.

\section{The Thermodynamic Integration Method for Estimating the Bayesian Evidence}
From the Sec.~\ref{sec:methods} we learned about power-posteriors in Eq.~(\ref{eq:power_posterior}) and the thermodynamic integral given in Eq.~(\ref{eq:thermoint}). We follow resources \citep{annis2019thermodynamic} for the derivation and discussion here. Here we derive the thermodynamic integral by considering the following expression implied by the 2nd Fundamental theorem of Calculus:
\begin{equation}\label{eqn:ti_identity}
    \mathrm{ln} \, \mathcal{Z}_{\beta=1}\left(\mathbf{d}\right) - \mathrm{ln} \, \mathcal{Z}_{\beta=0}\left(\mathbf{d}\right) = \int^1_0 \left(\frac{d\left(\mathrm{ln} \, \mathcal{Z}_\beta \left(\mathbf{d}\right) \right)}{d\beta}\right) \, d\beta = \int^1_0 \frac{1}{\mathcal{Z}_\beta \left(\mathbf{d}\right)} \frac{d \, \mathcal{Z}_\beta \left(\mathbf{d}\right)}{d\beta} d\beta.
\end{equation}
For a properly normalized prior, $\pi(\vec{\theta}$, $\mathrm{ln} \, \mathcal{Z}_{\beta=0} \left(\mathbf{d}\right) = 0$. This leaves the marginal likelihood at $\beta=1$ that we are interested in, the untempered $\mathrm{ln} \, \mathcal{Z} \left(\mathbf{d}\right)$. Now we can expand Eq.~\ref{eqn:ti_identity} as:
\begin{equation}
    \mathrm{ln} \, \mathcal{Z} \left(\mathbf{d}\right) = \int_0^1 \frac{\int \frac{d}{d\beta} \left[\pi\left(\vec{\theta}\right) \, \mathcal{L} \left(\mathbf{d}|\vec{\theta} \right)^\beta \right]\, d\vec{\theta} d\vec{\theta}}{\int \pi\left(\vec{\theta}\right) \, \mathcal{L}\left(\mathbf{d}|\vec{\theta} \right)^\beta \, d\vec{\theta}}.
\end{equation}
Suppressing parenthetical arguments on $\theta$ and $\mathbf{d}$ for clarity we can arrive at the following expression by taking the derivative in the numerator we then arrive at:
\begin{equation}
    \mathrm{ln} \, \mathcal{Z} = \int^1_0 \frac{\int \pi \, \, \, \left(\mathrm{ln} \, \mathcal{L}\right) \, \, \, \mathcal{L}^{\beta} d\theta}{\int \pi \mathcal{L}^{\beta} d\theta} d\beta.
\end{equation}
Using Bayes theorem we can replace the numerator and denominator with $\mathcal{P}_\beta = \pi \mathcal{L}^\beta / \mathcal{Z}_\beta$ to get:
\begin{equation}
    \mathrm{ln} \, \mathcal{Z} = \int^1_0 \frac{\int \mathcal{P}_\beta \, \left(\mathrm{ln} \, \mathcal{L}\right) d\theta}{\int \mathcal{P}_\beta   d\theta} d\beta = \int^1_0 \langle \mathrm{ln} \, \mathcal{L} \rangle_{\mathcal{P}_\beta} \, d\beta,.
\end{equation}
Thus, the logarithm of the untempered evidence is given by the one dimensional integral in Eq.~(\ref{eq:thermoint}).
where $\langle \mathrm{ln} \, \mathcal{L} \rangle_{\mathcal{P}_\beta}$ represents the average untempered log-likelihood under the measure described by the power-posterior distribution at $\beta$. Said in another way, this is the average untempered log-likelihood when drawing samples from the power-posterior distribution at $\beta$. We suppress this notation to write $\langle \mathrm{ln} \, \mathcal{L} \rangle_{\mathcal{P}_\beta} \equiv \langle \mathrm{ln} \, \mathcal{L} \rangle_\beta$ in the main-body of the text. Thus simulating from power-posterior distributions with $\beta$ between $0$ and $1$ provide a means to estimating the logarithm of the untempered evidence for the model and thus present a tractable way towards Bayesian model selection and comparison. This method is an ubiased estimator of the evidence provided that samples of $\langle \mathrm{ln} \, \mathcal{L} \rangle_\beta$ can be drawn in an unbiased manner from power-posteriors~\citep{carlson2016partition}.

It is convenient to describe additional derivatives of the thermodynamic integrand as they will be useful as references in the next section. In general, $\mathrm{n}^{\mathrm{th}}$ derivatives of the form $\mathrm{ln} \, \mathcal{Z}$ can be solved as~\citep{gradshteyn2015table}\footnote{Note that the solution in~\cite{gradshteyn2015table} has a minor typo, which we correct here.}:
\begin{equation}\label{eqn:gradshteyn_derivatives}
    \frac{d^n}{d\beta^n}\left( \mathrm{ln} \, \mathcal{Z} \right) = \sum_{k=1}^{n} \frac{(-1)^{(k+1)} {{n}\choose{k}}}{k \mathcal{Z}^k} \frac{d^n}{d\beta^n} \left(\mathcal{Z}^k\right).
\end{equation}
The first derivative, $n=1$, we have already solved as being $\langle \mathrm{ln} \, \mathcal{L} \rangle_\beta$. The next derivative, $n=2$, was found in~\cite{friel2014improving} as $\mathrm{Var}(\mathrm{ln} \, \mathcal{L})_\beta$ = $\langle (\mathrm{ln} \, \mathcal{L})^2\rangle_beta - \langle \mathrm{ln} \, \mathcal{L} \rangle^2_\beta$. This is the variance of the untempered log likelihood samples when drawn from the power-posterior at $\beta$. We solve the next derivative, $n=3$, as:
\begin{equation}\label{eqn:third_ti_deriv}
    \frac{d^3}{d\beta^3}\left( \mathrm{ln} \, \mathcal{Z}\right) = \langle \left(\mathrm{ln} \, \mathcal{L} \right)^3\rangle_\beta + 2 \langle \mathrm{ln} \, \mathcal{L} \rangle^3_\beta - 3 \langle \left(\mathrm{ln} \, \mathcal{L} \right)^2\rangle_\beta \langle \mathrm{ln} \, \mathcal{L}\rangle_\beta.
\end{equation}
We observe that the pattern is that the $\mathrm{n}^{\mathrm{th}}$ derivative of $\mathrm{ln} \, \mathcal{Z}$ follow the pattern of the $\mathrm{n}^{\mathrm{th}}$ cumulants~\citep{kardar2007statistical} of the power-posterior distribution at $\beta$~\cite{friel2014improving, lamont2019correspondence}. The term $\mathcal{Z}$ describes a partition function for the posterior distribution $\mathcal{P}$~\citep{carlson2016partition, lamont2019correspondence}. This relationship is helpful because we can make computation of values of high order derivatives more numerically stable since cumulants of order $\ge 2$ are shift-invariant~\cite{kardar2007statistical}. We can make the transformation of variables, $\widetilde{\mathrm{ln} \, \mathcal{L}} \equiv \mathrm{ln} \, \mathcal{L} - \mathrm{ln} \, \mathcal{L}_{\mathrm{max}}$ for every power-posterior before calculating high order derivatives of the $\mathrm{ln} \, \mathcal{Z}$ at power-posteriors. We have tested this on high-order derivatives and found it to be both accurate and numerically stable, although we have also found that the parallel-tempered \emph{emcee} sampler~\citep{emcee,vousden:2016} may not be stable enough to permit calculation of derivatives higher than order $3$.

For comparison, The thermodynamic integrand with the next two derivatives are shown in Fig.~\ref{fig:gooseneck_linear} for the permissive $\delta \phi$ prior choice ($\mathrm{log}_{10} A \in U[-10, -5.5], \, n \in U[-1, 3), \, f_0 \in U[10, 100]~Hz$) with a linear in $\beta$ scale. We also produce this plot in the logarithmic scale in~\ref{fig:gooseneck_log} where inspection of the curvature of the thermodynamic integrand is easier to see. These plots are helpful to inspect for places where the integrand may not be well sampled in $\beta$ and hence require additional inverse-temperatures~\cite{liu2016evaluating, de2011free, de2013comparison}. Of particular note is the instability in the second (third) subplot of Fig.~\ref{fig:gooseneck_log} where the second (third) derivative is not smooth in $\beta$. Even in the first subplot, where we expect the thermodynamic integrand to be smooth and monotonically increasing as $\beta$ goes from $0$ to $1$, there is some numerical instability at $\beta \sim 10^{-9}$. This implies, perhaps, the need for a better tempering sampler or bias-corrective terms in the sampling such as those found in~\cite{oates2017control,evans2019thermodynamic}. The instability is so slight however that we do not expect it to significantly impact the Bayes factor estimation, but it is a potential source of error in the numerical integration.

\begin{figure}[th]
\centering
\includegraphics[width=1.0\textwidth]{figs/chapter6/gooseneck_plots_linear.png}
\caption{The subplots of the thermodynamic integrand and subsequent derivatives of the thermodynamic integral. (\textit{Top}) The thermodynamic integrand when compared to the inverse-temperature $\beta$. The curve should be smooth and montonic, however it is very difficult to inspect the integrand on a linear $\beta$ scale. (\textit{Middle}) The second derivative of the logarithm of the evidence is the variance of the power-posterior at an inverse temperature $\beta$. There is some indication that an inflection point happens in the curvature of the integrand at high temperature. (\textit{Bottom}) The third derivative of the logarithm of the evidence is also the third-order cumulant of the power-posterior distributions at an inverse-temperature $\beta$. It is difficult to inspect the behavior of this derivative on the linear $\beta$ scale.}
\label{fig:gooseneck_linear}
\end{figure}

\begin{figure}[th]
\centering
\includegraphics[width=1.0\textwidth]{figs/chapter6/gooseneck_plots_log.png}
\caption{The subplots of the thermodynamic integrand and subsequent derivatives of the thermodynamic integral. (\textit{Top}) The thermodynamic integrand when compared to the inverse-temperature $\beta$. The curve should be smooth and montonic, however there is some indication at $\beta = 10^{-9}$ that this condition is not strictly met in the Markov Chain Monte Carlo simulation. (\textit{Middle}) The second derivative of the logarithm of the evidence is the variance of the power-posterior at an inverse temperature $\beta$. This function should also be smooth however there is some indication that at high temperature that the derivatives are not stable. (\textit{Bottom}) The third derivative of the logarithm of the evidence is also the third-order cumulant of the power-posterior distributions at an inverse-temperature $\beta$. Here we can see that the derivatives are not very sable or smooth. This may motivate moving our analysis to new multi-tempered samplers that are optimized for thermodynamic integration.}
\label{fig:gooseneck_log}
\end{figure}

\subsection{Numerical Quadrature}
The thermodynamic integral in Eq.~(\ref{eq:thermoint}) can be estimated through numerical quadrature rules such as the trapezoidal rule, or Simpson's rule. Because $\beta$ for thermodynamic integration are typically not uniformly distributed between $0$ and $1$, it is beneficial to consider integration rules that do not depend on equally spaced abscissa. A polynomial interpolant that does not make use of derivatives of the function or equally spaced abscissa is the Newton's divided difference interpolant, see~\cite{brun1953generalization, selmer1958numerical, abramowitz1965handbook} for examples of how to construct these polynomials. Other  interpolants, and thus integration rules, can be constructed, see ~\cite{abramowitz1965handbook} for examples.

The simplest rule that we consider here is the trapezoidal rule which can be written for thermodynamic integration as:
\begin{equation}
    \widehat{\mathrm{ln} \, \mathcal{Z}}_{\mathrm{Trapz}} = \sum_{i=0}^{N_\beta-1} \frac{1}{2} \left(\beta_{i+1} - \beta_i \right) \left(\langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i+1}} + \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i}} \right)
\end{equation}
Here $N_\beta$ represents the number of $\beta$ being summed over in the integration estimation. The error corrective term to the trapezoidal rule can be found by integrating the next-to-leading order Taylor polynomial correction~\citep{abramowitz1965handbook}, yielding:
\begin{equation}
    \widehat{\mathrm{ln} \, \mathcal{Z}}_{\mathrm{Trapz \, +}} \approx \widehat{\mathrm{ln} \, \mathcal{Z}}_{\mathrm{Trapz}} + \sum_{i=0}^{N_\beta-1} -\frac{1}{12} \left(\beta_{i+1} - \beta_i \right)^2 \left(f'(\beta_{i+1}) - f'(\beta_{i}) \right).
\end{equation}
Here $f'(\beta_i)$ represents the second derivative of $\mathrm{ln} \, \mathcal{Z}$ with respect to $\beta$. It was found in~\cite{friel2014improving} that this corresponds to the variance of the untempered log likelihood as drawn from the power-posterior at $\beta_i$.

Simpson's rule for unequally spaced abscissa under Newton's divided difference interpolation~\citep{easa1988area} is:
\begin{equation}
    \widehat{\mathrm{ln} \, \mathcal{Z}}_{\mathrm{Simps}} = \sum_{\mathrm{i\, is\, even}, \\ i=0}^{N_\beta-2} \frac{h_i + h_{i+1}}{6} \left [ A \, \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i}} + B \, \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i+1}} + C \, \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i+2}}\right ],
\end{equation}
for the expressions:
\begin{equation}
\begin{array}{lll}
     A &= \frac{(2h_i - h_{i+1})}{h_i} \\ \\ 
     B &= \frac{(h_i+h_{i+1})^2}{h_i h_{i+1}} \\ \\
     C &= \frac{(2h_{i+1} - h_i)}{h_{i+1}}. \\ \\
\end{array}
\end{equation}
Here  $h_i \equiv \beta_{i+1} - \beta_i$, and $h_{i+1} \equiv \beta_{i+2} - \beta_{i+1}$. The error corrective term for Simpson's rule can thus be solved in the same manner as for the trapezoidal rule and we find:
\begin{equation}
    \widehat{\mathrm{ln} \, \mathcal{Z}}_{\mathrm{Simps \, +}} \approx \widehat{\mathrm{ln} \, \mathcal{Z}}_{\mathrm{Simps}} + \sum_{\mathrm{i\, is\, even}, \\ i=0}^{N_\beta-2} \frac{1}{72}\left(\beta_{i+2} - \beta_{i} \right)^2 (\beta_i - 2\beta_{i+1} + \beta_{i+2})\frac{f''(\beta_{i+2}) - f''(\beta_i)}{\beta_{i+2} - \beta_i}.
\end{equation}
Here $f''(\beta_i)$ represents the third derivative of $\mathrm{ln} \, \mathcal{Z}$ with respect to $\beta$, which is in Eq.~\ref{eqn:third_ti_deriv}.

The cubic integration rule for unequally spaced abscissa under Newton's divided difference interpolation can be found in~\cite{brun1953generalization,selmer1958numerical,chambers1989estimating} or can be derived through the tools in~\cite{abramowitz1965handbook}. We also use the cubic integration rule, and in particular we use the form given in~\cite{chambers1989estimating}:
\begin{equation}
    \widehat{\mathrm{ln} \, \mathcal{Z}}_{\mathrm{cubic}} = \sum_{\mathrm{i\, is\, a \, multiple \, of \, 3}, \\ i=0}^{N_\beta-3} \frac{h_i + h_{i+1} + h_{i+2}}{12} \left [A \, \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i}} + B \, \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i+1}} + C \, \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i+2}} + D \, \langle \mathrm{ln} \, \mathcal{L} \rangle_{\beta_{i+3}}\right ],
\end{equation}
for expressions:
\begin{equation}
\begin{array}{llll}
     A &= \frac{3h_i^2 -h_{i+1}^2 +h_{i+2}^2 +2 h_i h_{i+1} - 2h_i h_{i+2}}{h_i (h_i + h_{i+1})} \\ \\
     B &= \frac{(h_i + h_{i+1} + h_{i+2})^2 (h_i + h_{i+1} - h_{i+2})}{h_{i} h_{i+1} (h_{i+1} h_{i+2})} \\ \\
     C &= \frac{(h_i + h_{i+1} + h_{i+2})^2 (h_{i+1} + h_{i+2} - h_i)}{h_{i+1} h_{i+2} (h_i + h_{i+1})} \\ \\
     D &= \frac{h_i^2 - h_{i+1}^2 +3 h_{i+2}^2 - 2h_i h_{i+2} + 2 h_{i+1} h_{i+2}}{h_{i+2}(h_{i+1} + h_{i+2})}.\\ \\
\end{array}
\end{equation}
Here we have defined $h_i \equiv \beta_{i+1} - \beta_{i}$, $h_{i+1} \equiv \beta_{i+2} - \beta_{i+1}$, and $h_{i+2} \equiv \beta_{i+3} - \beta_{i+2}$.

We exercise caution in describing the thermodynamic integral through a higher order polynomial quadrature rule as it may not be well described by polynomials. Thus there may be very little incentive for going to higher order polynomial rules as improved accuracy is not always guaranteed by going to higher order polynomial integration rules~\citep{epperson1987runge}. It is important to note that we treat the logarithm of the evidence as an unknown quantity which we are trying to infer the value of, and so we must treat the evidence as a random variable. Without prior information we should exercise caution when trusting one of these quadrature rules above the others. Our inference is more confident when these quadrature rules agree on the numerical value of the logarithm of the evidence.

Future studies may make use of Taylor series polynomials for unequally spaced abscissa, ratios of Taylor series polynomials through the Pad$\textrm{\'e}$ approximant for improved accuracy~\citep{press1992pade}, or other interpolant functions. Improvement in numerical integration for thermodynamic integration may also be improved by focusing on increasing the number of inverse-temperatures $\beta$ and/or by improved placement of $\beta$.

\subsection{Monte Carlo Error}
Here we follow the discussion from \cite{annis2019thermodynamic} who provide a heuristic for estimating the Monte Carlo error to estimating the thermodynamic integral as first given in \cite{friel2008marginal}. A variance of the thermodynamic integral estimator, $\widehat{\mathrm{ln} \, \mathcal{Z}}$, from Monte Carlo error can be found in two steps. First, calculate the thermodynamic integral for each sample of untempered log likelihoods drawn from the power-posterior at $\beta$. For $N$ samples drawn from each power-posterior this generates $N$ thermodynamic integral values. The integration should be done relative to the numerical quadrature technique that one is trying to estimate the Monte-Carlo error for. This represents the sample variance of the thermodynamic integration. The variance of the mean value of the logarithm of the evidence can be calculated via:
\begin{equation}
    \sigma^2_{\mathrm{MC}} = \frac{1}{N} \sigma^2_{\mathrm{sample}}.
\end{equation}
Here, $\sigma^2_{\mathrm{MC}}$ represents the Monte-Carlo variance for the thermodynamic integration estimator while $\sigma^2_{\mathrm{sample}}$ is the sample variance and $N$ represents the number of available samples. See Fig.~\ref{fig:ti_monte_carlo_error} for a visualization of this procedure.

\begin{figure}[th]
\centering
\includegraphics[width=0.9\columnwidth]{figs/chapter6/ti_monte_carlo_error.png}
\caption{The first subplot denotes the untempered log-likelihood samples when drawn from the power-posteriors at $\beta$. The expectation value of the untempered log-likelihood when drawn from these power-posteriors is the thermodynamic integrand and is plotted in red. The thermodynamic integral over all geometric paths given from the samples is drawn in the second subplot. The sample-log-integral distribution is approximately a Gaussian distribution. The standard error of the mean value of the log evidence is given by the sample standard deviation divided by the square root of the number of samples. The $90 \%$ confidence interval on the sample distribution in the log-evidence is drawn in dashed orange lines. The $90\%$ confidence region from this standard error is shaded in red. The final subplot is a zoom-in on this $90 \%$ confidence region showing the error estimate on the thermodynamic integral due to Monte Carlo sampling. This, when combined with the convergence error, is used in the final error estimate on the log-evidence of the thermodynamic integral.}
\label{fig:ti_monte_carlo_error}
\end{figure}

Repeated runs where the random seed for the Markov-Chain Monte Carlo analysis was changed has shown that the variance estimate from presented in~\cite{annis2019thermodynamic} is a plausible confidence interval estimate for Monte Carlo error. It has also shown good agreement with the steppingstone Monte Carlo error estimate which uses the same samples as those in thermodynamic integration.

\subsection{Convergence Error}
The procedure of estimating the marginal likelihood from power-posterior simulation requires that the power-posteriors all converge to the proper distribution. To first order, this requires inspection of the thermodynamic integrand over the course of the MCMC analysis. To next order, this would require that sequential cumulants of the power-posterior also stabilizes. In the limit that the MCMC analysis has converged all of the power-posterior distributions will be stationary as a function of MCMC iteration. During the course of the study we did not fully investigate the stationarity of the full power-posterior distribution through inspection of all of the cumulants, but rather focused on the stationarity of the thermodynamic integrand across each temperature. This resulted in investigating the stationarity of the the thermodynamic integral as well.

An accurate depiction of the power-posterior distribution for a particular temperature requires that all of the samples be independent and identically distributed samples (this is sometimes called \textif{i.i.d.} in the statistics literature)~\cite{annis2019thermodynamic}. Gathering independent and identically distributed samples can be done by calculating the autocorrelation length of the MCMC chains from a particular temperature. In practice, PyCBC Inference calculates the autocorrelation length of all of the temperature chains and uses the largest autocorrelation length as the autocorrelation length for all temperatures. This is a safe and conservative practice for ensuring that samples drawn from the MCMC simulation are not correlated. Thus, to track the thermodynamic integrand at various iterations in the MCMC simulation we divide the MCMC analysis into $12$ equally spaced partitions  based on the number of MCMC iterations that the analysis has undergone. In practice any number of partitions will do, but it is computationally intensive to sample more partitions. The partitions do not need to be equally spaced in MCMC iterations but we find equally spaced partitions to be useful for visualization of the progression of the thermodynamic integrand. Using this number of partitions, each partition is segmented in half, where the first half is discarded as burn-in samples, and the autocorrelation length is calculated from the remaining samples. Then independent samples are drawn from this segment spaced out by autocorrelation length. This is the generic procedure of the $\mathrm{n_{acl}}$ algorithm implemented in PyCBC for drawing independent samples from the Markov chains. The partitioning is shown in Fig.~\ref{fig:nacl_segments}. Having drawn independent samples from $12$ segments of the MCMC analysis we can visually inspect the stability of the thermodynamic integrand at $12$ iterations in the MCMC analysis. We can also inspect the convergence of the thermodynamic integral. When the logarithm of the evidence has converged to $\mathcal{O}(10^{-2}$ accuracy, we usually consider the power-posteriors to have converged to their final distribution. We base our inference on convergence on the worst quadrature method, the trapezoidal rule for thermodynamic integration, as a means to be conservative. Higher order quadrature rules can converge more rapidly than the trapezoidal rule, however, the more closely aligned the numerical quadrature estimates the more confidence we can attain in the accuracy of the thermodynamic integration. Figure~\ref{fig:integrand_convergence} shows the progression of the convergence of the thermodynamic integrand as a function of the MCMC iteration. Figure~\ref{fig:integral_convergence} shows the convergence rate of the thermodynamic integral as a function of the MCMC iteration for a variety of integration techniques.

\begin{figure}[th]
\centering
\includegraphics[width=0.9\columnwidth]{figs/chapter6/convergence_segmentation_lvc_sim.png}
\caption{The partitioning of the MCMC analysis to check on the convergence of the thermodynamic integrand and the thermodynamic integration. The dark-green bar at the top represents all of the samples collected by the MCMC analysis. This segment is divided into 12 segments represented by the light gray lines. The light-green segments represent chunks that independent samples can be drawn from. The dark region represents samples discarded as burn-in samples for the MCMC. The dark grey region represents data that is ahead of the chunk and thus not used in drawing independent samples for that chunk. Chunk $12$ produces the identical samples as drawing independent samples according to the $\mathrm{n_{acl}}$ algorithm from PyCBC at the end of the analysis.}
\label{fig:nacl_segments}
\end{figure}

\begin{figure}[th]
\centering
\includegraphics[width=1.0\columnwidth]{figs/chapter6/lsc_sim_integrand_progress.png}
\caption{The convergence of the thermodynamic integrand for the unconstrained $\delta \phi$ prior choice on the $p$-$g$ mode instability model. The Iteration-Start denotes the point is taken from a segment beginning with that MCMC iteration and ending with the MCMC iteration denoted as Iteration-End. These iterations correspond to the segments found in Fig.~\ref{fig:nacl_segments}. The logarithm of the evidence is shown also in the figure caption, and it can be noted that as the MCMC analysis progresses the integral converges to a set value. The thermodynamic integrand can be visually seen to converge to the S-like curve seen in the figure. Early in the analysis the curve can be mishaped as the power-posteriors have not all converged. Experience has told us tha the power-posteriors that take the longest to converge tend to be in the region where the average log likelihood changes rapidly. Here this is in the region between $\beta$ $\in$ ($10^{-2} - 1$).}
\label{fig:integrand_convergence}
\end{figure}

\begin{figure}[th]
\centering
\includegraphics[width=1.0\columnwidth]{figs/chapter6/lvc_sim_evidence_convergence.png}
\caption{The convergence of the thermodynamic integral for the unconstrained $\delta \phi$ prior choice on the $p$-$g$ mode instability model as a function of the MCMC iteration. These choice of points of iterations correspond to the segments found in Fig.~\ref{fig:nacl_segments}. As the analysis progresses the logarithm of the evidence from all methods tend towards a fixed value.}
\label{fig:integral_convergence}
\end{figure}

The absolute value of the difference between the last two thermodynamic integration estimates from this partitioning are then used as the standard deviation of the error for the log evidence due to convergence error, $\sigma_{\mathrm{convergence}}$:
\begin{equation}
    \sigma_{\mathrm{convergence}} \sim  \, \mid \mathrm{ln} \, \mathcal{Z}_{\mathrm{partition \,} N} - \mathrm{ln} \, \mathcal{Z}_{\mathrm{partition \,} N-1} \mid
\end{equation}
This provides a rough estimate for ensuring that we do not terminate the MCMC analysis too early and thus give ourselves overconfidence about the true value of logarithm of the evidence. Previous analyses such as those in \cite{de2018tidal} did not make use of this technique and the analysis is terminated based on the number of independent samples collected in the posterior distribution. This is an insufficient metric for inference analyses based on Bayesian model selection through multi-tempered MCMC simulation. We use this estimate for $\sigma_{\mathrm{convergence}}$ in Eq.~\ref{eq:errorprop}.

During the development of this technique a similar technique based on a moving-block bootstrap method was developed in~\cite{Russel:2018pqv} for error analysis of the logarithm of the evidence from the thermodynamic integration method. We have not investigated this technique thoroughly to compare its performance with our own method.

\subsection{Temperature Placement Bias}
The placement of inverse-temperatures $\beta$ affects the results of the numerical integration for the evidence\citep{lartillot2006computing, xie2010improving}. This study has a particular bias at the low-end of $\beta \to 0$ because it does not include $\beta = 0$ in the numerical integration. This tail-end bias at small $\beta$ is much, much lower than the resolution of the Monte Carlo error and the convergence error and so it is irrelevant to the final results. However, future practice should always include $\beta =0$.

Research into the proper placement of $\beta$ is ongoing in the field of Statistics~\citep{annis2019thermodynamic}. We followed the suggestions in~\cite{liu2016evaluating} on placing temperatures where the thermodynamic integrand changed rapidly. With $51$ inverse-temperatures it is incredibly unlikely that the main results of the Bayes factors being $\sim 0.7$ are biased by discretization error outside of the statistical uncertainties. This is implied by the agreement of the results between thermodynamic integration, the steppingstone, and the Savage-Dickey density ratio methods. In Fig.~\ref{fig:lvc_sim_log_evidence_distr} there is some slight disagreement on the exact value of the marginal log likelihood, but it seems to cancel out in the Bayes factor since all models share the same temperature ladder (see Fig~\ref{fig:lvc_sim_uni_ceos_bayes_distr}).

One method for improving the temperature ladder would be to use the method of~\cite{friel2014improving} by using the intersection of the slopes of the thermodynamic integrand from two adjacent power-posteriors as a new position for additional temperatures. We could go further by using the intersections of higher-order polynomials using the expressions for the derivatives of the thermodynamic integrand. However, this is not likely to be fruitful in this study as our error at the moment appears dominated by Monte Carlo error and convergence error. Future studies using multi-tempering techniques may make use of this method in refining temperature placement.

It is our opinion that the placement of inverse-temperatures, if it cannot be solved analytically, should be considered a question of inference. That is to say, given a prior belief on an appropriate distribution on the placement of inverse-temperatures, how should one adjust the placement of inverse-temperatures given the results of the numerical quadrature routine? Bayesian quadrature is a promising area of research meant to address optimal placement of abscissa for numerical integration. It is likely that the thermodynamic integration method would likely greatly benefit from this sort of an approach. See~\cite{diaconis1988bayesian} for an initial formulation and~\cite{briol2015probabilistic} for a modern perspective, especially with respect to thermodynamic integration.

\section{The Steppingstone Method for Estimating the Bayesian Evidence}
The steppingstone method is very similar in many respects to thermodynamic integration in that it requires multiple inverse-temperatures between $0$ and $1$ to calculate. The motivation for steppingstone is that it uses importance sampling between adjacent temperatures to estimate the contribution to the marginal likelihood $\mathcal{Z}$ at each interval $\beta_{i-1}$-$\beta_i$. Before we derive the steppingstone method we provide a brief, but useful derivation of another often used identity, called the harmonic mean estimator for the evidence. For the following section we suppress use of $\vec{\theta}$ and $\mathbf{d}$.

For the derivation of the harmonic mean estimator we follow a simplified version of the derivation presented in \citep{newton1994approximate}. From the definition of the marginal likelihood we can write:
\begin{equation}
    \frac{1}{\mathcal{Z}} = \frac{1}{\int \pi \, \mathcal{L} \, d\theta}.
\end{equation}
Since we only deal with proper priors we can substitute the numerator with $\int \pi d\theta = 1$. This gives:
\begin{equation}
    \frac{1}{\mathcal{Z}} = \frac{\int \pi \, d\theta}{\int \pi \, \mathcal{L} \, d\theta}.
\end{equation}
Now we multiply both the numerator and denominator by $\mathcal{P}/\mathcal{P}$ to get:
\begin{equation}
    \frac{1}{\mathcal{Z}} = \frac{\int \frac{\pi}{\mathcal{P}} \mathcal{P} \, d\theta}{\int \frac{\pi \, \mathcal{L}}{\mathcal{P}}\mathcal{P} \, d\theta}.
\end{equation}
Which we simplify using Bayes theorem to substitute out for $1/\mathcal{P}$ to give:
\begin{equation}
    \frac{1}{\mathcal{Z}} = \frac{\int \frac{\pi \mathcal{Z}}{\pi \mathcal{L}} \mathcal{P} \, d\theta}{\int \frac{\pi \, \mathcal{L} \mathcal{Z}}{\pi \mathcal{L}}\mathcal{P} \, d\theta}.
\end{equation}
Cancelling out terms of $\pi$ and moving terms of $\mathcal{Z}$ out of the integral to cancel, this gives:
\begin{equation}\label{eqn:HME}
    \frac{1}{\mathcal{Z}} = \frac{\int \frac{1}{\mathcal{L}} \mathcal{P} \, d\theta}{\int \mathcal{P} \, d\theta} = \int \frac{1}{\mathcal{L}} \mathcal{P} \, d\theta.
\end{equation}
Therefore we can express the inverse of the evidence as:
\begin{equation}
    \frac{1}{\mathcal{Z}} = \langle \mathcal{L}^{-1} \rangle_{\mathcal{P}},
\end{equation}
which is to say that the inverse of the evidence is given as the average value of the inverse of the likelihood when sampled from the measure defined by the posterior distribution. This is the harmonic mean estimator of the evidence, and although it is correct in theory, it typically misbehaves numerically and computationally. It is also worth noting as~\cite{xie2010improving} points out that the harmonic mean estimator is not sensitive to the prior distribution which runs contradictory to the heart of Bayesian model comparison. We will use this identity in the derivation of the steppingstone estimator.

We follow~\cite{annis2019thermodynamic} in the derivation of the steppingstone estimator. Recall from Eq. \ref{eq:thermoint} that the marginal likelihood can be expressed as:
\begin{equation}
    \mathrm{ln} \, \mathcal{Z} = \mathrm{ln} \, \mathcal{Z}_{\beta=1} - \mathrm{ln} \, \mathcal{Z}_{\beta=0},
\end{equation}
which is equivalent to:
\begin{equation}
    \mathcal{Z} = \frac{\mathcal{Z}_{\beta=1}}{\mathcal{Z}_{\beta=0}}.
\end{equation}
For, say, a hundred equally spaced temperatures between $0$ and $1$ this motivates the following re-expression:
\begin{equation}
    \mathcal{Z} = \frac{\mathcal{Z}_{\beta = 0.01}} {\mathcal{Z}_{\beta = 0}}  
    \times \frac{\mathcal{Z}_{\beta = 0.02}} {\mathcal{Z}_{\beta = 0.01}}
    \times \ldots \times \frac{\mathcal{Z}_{\beta = 0.99}} {\mathcal{Z}_{\beta = 0.98}} \times \frac{\mathcal{Z}_{\beta = 1}} {\mathcal{Z}_{\beta = 0.99}}.
\end{equation}
The general form for this is:
\begin{equation}\label{eqn:ssa_prod_series}
     \mathcal{Z} = \prod_{i=1}^{N_\beta} \frac{\mathcal{Z}_{\beta_{i}}}{\mathcal{Z}_{\beta_{i-1}}}.
\end{equation}
Here we use the ordering on $\beta$, as $\beta_0=0 < \beta_1 < ... < \beta_{N_\beta -1} < \beta_{N_\beta} = 1$. Finally, then, consider the evidence for the  power-posterior at inverse-temperature $\beta_i$ given as:
\begin{equation}
    \mathcal{Z}_{\beta_i} = \int \pi \mathcal{L}^{\beta_i} \, d\theta.
\end{equation}
We now divide by $1$ via $\int \pi \, d\theta$ and multiply by $1$ via $\mathcal{P}_{\beta_{i-1}} \, / \, \mathcal{P}_{\beta_{i-1}}$ in the numerator and denominator to get:
\begin{equation}
    \mathcal{Z}_{\beta_i} = \left(\int \frac{\pi \mathcal{L}^{\beta_i}}{\mathcal{P}_{\beta_{i-1}}} \mathcal{P}_{\beta_{i-1}} \, d\theta \right )\bigg / \left( \int \frac{\pi}{\mathcal{P}_{\beta_{i-1}}} \mathcal{P}_{\beta_{i-1}} \, d\theta \right).
\end{equation}
Using Bayes theorem we substitute $\mathcal{P}_{\beta_{i-1}}$ = $(1/\mathcal{Z}_{\beta_{i-1}}) \, \pi \, \mathcal{L}^{\beta_{i-1}}$ to get:
\begin{equation}
    \mathcal{Z}_{\beta_i} = \left (\int \frac{\pi \mathcal{L}^{\beta_i} \mathcal{Z}_{\beta_{i-1}}}{\pi \mathcal{L}^{\beta_{i-1}}} \mathcal{P}_{\beta_{i-1}} \, d\theta \right) \bigg / \left(\int \frac{\pi \mathcal{Z}_{\beta_{i-1}}}{\pi \mathcal{L}^{\beta_{i-1}}} \mathcal{P}_{\beta_{i-1}} \, d\theta\right).
\end{equation}
Terms of $\mathcal{Z}_{\beta_{i-1}}$ are independent of $\theta$ and so can be moved out of the integral where they cancel, and we can cancel terms of $\pi$ to get:
\begin{equation}
    \mathcal{Z}_{\beta_i} = \left(\int \frac{ \mathcal{L}^{\beta_i} }{\mathcal{L}^{\beta_{i-1}}} \mathcal{P}_{\beta_{i-1}} \, d\theta\right) \bigg / \left(\int \frac{1}{ \mathcal{L}^{\beta_{i-1}}} \mathcal{P}_{\beta_{i-1}} \, d\theta\right).
\end{equation}
Finally we recognize that in the denominator we have Eq.~\ref{eqn:HME} for the inverse of the evidence at the inverse-temperature $\beta_{i-1}$, and in the top we can simplify terms so as to get:
\begin{equation}
    \mathcal{Z}_{\beta_i} = \mathcal{Z}_{\beta_{i-1}} \int \mathcal{L}^{\beta_i - \beta_{i-1}}  \mathcal{P}_{\beta_{i-1}} \, d\theta.
\end{equation}
Thus we arrive at the key ingredient for the steppingstone estimator:
\begin{equation}\label{eqn:ssa_identity}
    \frac{\mathcal{Z}_{\beta_i}}{\mathcal{Z}_{\beta_{i-1}}} = \int \mathcal{L}^{\beta_i - \beta_{i-1}}  \mathcal{P}_{\beta_{i-1}} \, d\theta = \langle \mathcal{L}^{\beta_i - \beta_{i-1}} \rangle_{\mathcal{P}_{\beta_{i-1}}}.
\end{equation}
This can be uncomfortably summarized as saying, in the interval of inverse-temperatures between $\beta_{i-1}$ and $\beta_i$, the ratio of the evidences between successive inverse-temperatures is given by the average of the likelihood raised to the difference in the inverse-temperatures when samples for the likelihood are drawn from the power-posterior distribution for the smaller of the  inverse-temperatures in the inverse-temperature interval. We suppress some of the notation in Eq.~(\ref{eqn:ssa_identity}) such that $\langle \mathcal{L}^{\beta_i - \beta_{i-1}} \rangle_{\mathcal{P}_{\beta_{i-1}}} \equiv \langle \mathcal{L}^{\beta_i - \beta_{i-1}} \rangle_{\beta_{i-1}}$ We finally combine Eq.~(\ref{eqn:ssa_identity}) into Eq.~(\ref{eqn:ssa_prod_series}) to achieve the steppingstone estimator for the evidence:
\begin{equation}\label{eqn:steppingstone}
    \mathcal{Z} = \prod_{i=1}^{N_\beta} \langle \mathcal{L}^{\beta_i - \beta_{i-1}} \rangle_{\beta_{i-1}}. 
\end{equation}
Some care needs to be taken in the implementation of Eq.~(\ref{eqn:steppingstone}) as the form presented is not numerically stable and we often must use the log likelihood and log evidence in place of the likelihood and the evidence. A numerically stable form of the logarithm of Eq.~(\ref{eqn:steppingstone}) is presented in~\cite{xie2010improving}. It is noted to exhibit some level of bias as an estimator of the marginal likelihood due to its logarithmic form. The bias is noted to be small, and it was shown in~\cite{xie2010improving} that the steppingstone estimator typically outperforms the trapezoidal rule for thermodynamic integration in terms of accuracy. This bias can be mitigated by increasing the number of temperatures and/or improving their placement between $0$ and $1$~\citep{xie2010improving}.

For the steppingstone estimator we can use the same samples as for the thermodynamic integration method. Optimal temperature placement for the steppingstone estimator is an active area of research~\citep{annis2019thermodynamic}.

\subsection{Monte Carlo Error}
In~\cite{xie2010improving} there is an expression for the estimated variance of the logarithmic steppingstone estimator using an approximation method called the $\delta$ method~\citep{oehlert1992note}. The expression in~\cite{xie2010improving} for the variance of the logarithm of the evidence is however not presented in a numerically stable version. We use a numerically stabilized version of the variance estimator in our study. We have found the variance estimate from the $\delta$ method is typically comparable to the thermodynamic integration method's Monte-Carlo error. Repeated runs where the random seed for the Markov-Chain Monte Carlo analysis was changed has shown that the variance estimate from presented in~\cite{xie2010improving} is a plausible confidence interval estimate for Monte Carlo error.  

\subsection{Convergence Error}
The method for calculating the error on the steppingstone estimator due to convergence error is algorithmically identical to the thermodynamic integration method.

\subsection{Temperature Placement Bias}
At the current time optimal placement of inverse-temperatures $\beta$ remains an active area of research~\citep{annis2019thermodynamic}. Due to the large number of inverse-temperatures $\beta$ we do not believe that increased number of $\beta$ nor more optimal placement of $\beta$ would significantly alter the results of this study. It is regrettably a potential source of bias in our analysis that we cannot fully quantify, although heuristically we believe the bias to be small.

\section{The Savage-Dickey Density Ratio Method}\label{sec:sddr_derivation}
The Savage-Dickey density ratio method for Bayes factor calculation requires consideration of two models, wherein one model is nested in the other model. We derive the method and explain its limitations following~\cite{wagenmakers2010bayesian}. We can consider two models that are parametrized in the following way:
\begin{eqnarray}
    \pi \left(\vec{\theta}_{\mathrm{simple}}|\mathrm{H}_{\mathrm{simple}}\right)  &\equiv& \pi \left(\left \{\mathcal{M}, \eta, \chi_{\mathrm{eff}}, \tilde{\Lambda}, \ldots  \right \} | \mathrm{H}_{\mathrm{simple}} \right)\\
    \pi \left(\vec{\theta}_{\mathrm{complex}}| \mathrm{H}_{\mathrm{complex}}\right) &\equiv& \pi \left(\left \{\mathcal{M}, \eta, \chi_{\mathrm{eff}}, \tilde{\Lambda}, \ldots , A, f_0, n \right \}| \mathrm{H}_{\mathrm{complex}}\right).
\end{eqnarray}
In the $p$-$g$ mode instability parametrization setting $A = 0$, effectively reduces the parameter space from the complex parameter space including $p$-$g$ mode parameters to the simple parameter space denoted as the standard TaylorF2 parameter space in the main text. We abbreviate the notation by writing the prior under the simple hypothesis  as $\pi_{!\mathrm{NL}} \left(\psi \right)$ and the prior under the more complex hypothesis as $\pi_{\mathrm{NL}} \left(\psi, A\right)$. Here the dependence on hypotheses is denoted by the subscript !NL or NL, and $\psi$ denotes all parameters that are not $A$. The parameters $\psi$ can be considered for the purposes of this derivation to be nuisance parameters. In order for the Savage-Dickey Density Ratio method to hold for the case here we require the following expression be satisfied:
\begin{equation}\label{eqn:sddr_condition}
    \lim_{A \to 0} \pi_{\mathrm{NL}} \left(\psi | A\right) = \pi_{\mathrm{!NL}}\left(\psi\right).
\end{equation}
In essence, this is stating that setting $A = 0$ reduces the prior parameter space from including $p$-$g$ mode parameters (and they're potential effect on the likelihood function) down to the TaylorF2 parameter space with point-particle parameters and linear tidal parameters. These conditions are in fact satisfied by setting $A=0$ and so we can present the Bayes factor as:
\begin{equation}
    \mathcal{B}^{\mathrm{NL}}_{!\mathrm{NL}} = \frac{\mathcal{Z}_{\mathrm{NL}}(\mathbf{d})}{\mathcal{Z}_{\mathrm{!NL}}(\mathbf{d})}.
\end{equation}
Now, we also know that the denominator can be expressed according to:
\begin{equation}\label{eqn:evidence_sub_sddr}
    \mathcal{Z}_{\mathrm{!NL}}(\mathbf{d}) = \int \pi_{\mathrm{!NL}}\left(\psi\right) \, \mathcal{L}_{\mathrm{!NL}} \left(\mathbf{d} | \psi \right)  d\psi.
\end{equation}
Since the models are nested, the prior (likelihood) under the NL hypothesis at $A=0$ is equivalent to the prior (likelihood) under the !NL hypothesis. That is to say:
\begin{equation}\label{eqn:sddr_sub_eqs1}
\pi_{NL}\left(\psi, A=0\right) = \pi_{!NL}\left(\psi \right) \end{equation}
and
\begin{equation}
\label{eqn:sddr_sub_eqs2}
\mathcal{L}_{NL}\left(\mathbf{d}|\psi, A=0\right) = \mathcal{L}_{!NL}\left( \mathbf{d} | \psi \right).
\end{equation}
If we substitute Eqs.~\ref{eqn:sddr_sub_eqs1} and \ref{eqn:sddr_sub_eqs2} into Eq.~\ref{eqn:evidence_sub_sddr} we get:
\begin{equation}
    \mathcal{Z}_{\mathrm{!NL}}(\mathbf{d}) = \int \pi_{\mathrm{NL}}\left(\psi, A=0\right) \, \mathcal{L}_{\mathrm{NL}} \left(\mathbf{d} | \psi, A=0 \right)  d\psi.
\end{equation}
Integrating this over all $\psi$, leaves the $A=0$ unintegrated over leaving us with $\mathcal{Z}_{\mathrm{!NL}} = \mathcal{L}_{\mathrm{NL}} \left(\mathbf{d} | A=0 \right)$. Using Bayes theorem, we can rewrite $\mathcal{L}_{\mathrm{NL}} \left(\mathbf{d} | A=0 \right) = [\mathcal{P}_{\mathrm{NL}}(A=0 | \mathbf{d}) \, \mathcal{Z}_{\mathrm{NL}}(\mathbf{d})] / \pi_{\mathrm{NL}} (A=0)$. This leaves us with:
\begin{equation}
    \mathcal{Z}_{\mathrm{!NL}}\left(\mathbf{d}\right) = \frac{\mathcal{P}_{\mathrm{NL}}\left(A=0 | \mathbf{d}\right) \mathcal{Z}_{\mathrm{NL}} \left(\mathbf{d} \right)} {\pi_{\mathrm{NL}} \left(A=0\right)},
\end{equation}
and thus:
\begin{equation}
    \mathcal{B}^{\mathrm{NL}}_{\mathrm{!NL}} = \frac{\pi_{\mathrm{NL}}\left(A=0\right)}{\mathcal{P}_{\mathrm{NL}}\left(A=0 | \mathbf{d}\right)}.
\end{equation}
The more appropriate manner to write this expression requires the use of limits as expressed in Eq.~(\ref{eq:sddr_bayes_factor}). As outlined in the main-body, we can substitute with little risk $A=0$ with $A=10^{-10}$. A generalized Savage-Dickey density ratio test that makes fuller use of Eq.~\ref{eqn:sddr_condition} makes an additional correction factor to this Bayes factor~\citep{verdinelli1995computing}, but it does not play a role in our analysis since our models are nested.

\subsection{Histogram Methods for Estimating the Savage-Dickey Density Ratio Test}
\subsection{A Gaussian Kernel Density Estimator for the Savage-Dickey Density Ratio Test}
\subsection{A Polynomial Spline Density Estimator for the Savage-Dickey Density Ratio Test}
