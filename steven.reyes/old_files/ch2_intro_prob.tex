\section{Rules of Probability}
Here we simply outline a few simple rules for probability that will help make the following discussion simpler to understand. Perhaps the simplest rule of probability is that given a series of possible outcomes, the sum of the probabilities of the outcome must equal unity. This is expressed as:
\begin{equation}\label{eqn:probsumdiscrete}
    \sum_{i=1}^{N} p_i = 1.
\end{equation}
Here $p_i$ represents the probability mass function, or more simply, the probability of the \textit{$i^{th}$} outcome, given $N$ possible events. If the random variable is continuous then we simply express this as an integral:
\begin{equation}\label{eqn:probsumcontinuous}
    \int p \left( x \right) dx = 1.
\end{equation}
Where $p(x)$ represents the probability density function of a particular outcome $x$.

Finally, we describe a few rules of adding probabilities, multiplying probabilities, and conditional probability. The probability of two events occurring is $p(A \textrm{ and } B)$:
\begin{equation}\label{eqn:probAandprobB}
    p\left(A\textrm{ and }B\right) \equiv p(A,B) = p(A) \, p(B|A) = p(B) \, p(A|B)
\end{equation}
This new term here $p(B|A)$ is to be interpreted as the probability that event B occurs given that A has occurred, and similarly, $p(A|B)$ means the probability that event A occurs given that B has occurred.

This last expression from Eq. \ref{eqn:probAandprobB} motivates the theorem known as Bayes Theorem, which we will express as follows:
\begin{equation} \label{eqn:BayesTheorem_basic}
     p(H|D) = \frac{p(H) \, p(D|H)}{p(D)}.
\end{equation}
In this formulation we have written, the probability of the hypothesis given the data, $p(H|D)$, is sometimes called the posterior probability. The probability of the hypothesis being true is $p(H)$, and is often called the prior probability since it is what we believe prior to looking at the data. The probability of the data given the hypothesis, $p(D|H)$, which is called the likelihood. And finally we have the probability of obtaining the data, $p(D)$. We will devote a large amount of time in this work towards Bayes Theorem and its usefulness in conducting statistical inferences. 

\section{A Brief Introduction to Frequentist Statistics}
Frequentist statistics is the perspective that probabilities represent the frequency in which a random process will generate a particular result in the long-term. Frequentist statistics is a perspective that may be taken in the event that one expects to be able to examine the long term behavior of a particular random process. Furthermore, Frequentist statistical inference will place emphasis on the likelihood term, $p(D|H)$, from Eq.~\ref{eqn:BayesTheorem_basic}. A choice on the probability of $p(H)$ is not made explicitly.

\subsection{Null Hypothesis Significance Testing: Statistical Significance}
In Frequentist statistics we are often concerned with evaluating whether our particular data can be well-explained by a particular hypothesis. Notably, in science we are concerned whether the data we observe have a high probability of being generated by a null hypothesis, often that the data are not significantly different from what we might expect from noise processes alone.

The p-value represents the probability that the data (datum) is generated by the data-generating process in the null hypothesis. If we choose some confidence threshold  $\alpha$ that is sufficiently small, we can gain confidence in rejecting the null hypothesis if the p-value is smaller than this $\alpha$. This term $\alpha$ is sometimes called the significance level. Typically the p-value concerns the value of an estimator $x$ being greater (less) than some critical value, where the critical value is chosen so as to produce the right (left)-tailed probability $\alpha$. In the case of a right-tailed p-value we can write this in the following notation:
\begin{equation}\label{eqn:p_value}
    p(x \geq X | H_{0}) < \alpha.
\end{equation}
Typical choices of $\alpha$ are chosen to be small so as to reduce an incorrect rejection (or retention) of the null hypothesis $H_0$. One of the downsides of the p-value is that it states the probability of obtaining some value of $x$ given that the null hypothesis is true. This requires that the null hypothesis is a well-thought-out hypothesis and that it is an adequate model to test against in an experiment. In many areas of particle physics an $\alpha$ of $10^{-7}$ is often chosen as an acceptable threshold to guard against false rejection of the null hypothesis. In general however, a choice of statistical significance threshold $\alpha$ is arbitrary.

\subsubsection{Controlling Error Rates}

In light of choosing a particular $\alpha$ threshold by which we choose to reject or retain the null hypothesis, we inevitably open ourselves up to error rates if we happen across a very improbable datum or data set that \textbf{is} generated by the null hypothesis.

Incorrect inferences in Frequentist statistical significance hypothesis testing are typically called \textit{Type I} (false positive) errors and \textit{Type II} (false negative) errors. Correct inferences are usually then called true positive inferences and true negative inferences. If the null hypothesis is true, and we set our significance threshold to $\alpha$, and we reject the null hypothesis, then we are making a false positive error. The probability of doing this is:
\begin{equation}
    p\left(\mathrm{rejecting \, H_{0}}| \mathrm{H_{0}} = \mathrm{True}\right) \equiv \alpha
\end{equation}
If we assume that the null hypothesis is true and choose $\alpha$ threshold for our p-value significance test, then the probability of failing to reject the null hypothesis when the null hypothesis is true is a true negative inference. The probability of doing this is:
\begin{equation}
    p\left(\mathrm{failing \, to \, reject \, H_{0}}| \mathrm{H_{0}} = \mathrm{True}\right) = 1 - \alpha
\end{equation}
 If we fail to reject the null hypothesis, but the null hypothesis is false, then we make a false negative error. We can ascribe the probability of doing this as:
 \begin{equation}
    p\left(\mathrm{rejecting \, H_{0}}| \mathrm{H_{0}} = \mathrm{False}\right) \equiv \beta
 \end{equation}
 The probability of rejecting the null hypothesis when the null hypothesis is false is given as:
 \begin{equation}
    p(\mathrm{failing \, to \, reject \, H_{0}}| H_{0} = \mathrm{False})= 1 - \beta    
 \end{equation}
 This probability, $1- \beta$ is sometimes called the \textit{power} of the statistical inference, since it tells us how efficacious the hypothesis test is in rejecting the null hypothesis when we should reject it (e.g. there is a gravitational wave present in the data and we find it). Hence we see that choosing an $\alpha$ level for rejecting the null hypothesis permits us to protect from Type I errors, false positives, and gives us some understanding on the probability of correctly assessing true negatives. However choosing an $\alpha$ level does not offer much in the way of protecting from Type II errors, false negatives, nor much control over assessing true positives.

Finally, the last topic we describe is the problem of analyzing the data multiple times for statistical significance. This is sometimes called the problem of multiple comparisons or the look-elsewhere-effect. The problem can be simply explained under the following example. Say that one conducts a null-hypothesis statistical significance test at an $\alpha = 0.05$ threshold. . This opens the possibility of reporting a statistically significant result without properly attributing the number of trials conducted on the data. This is a form of misreporting in statistics and is sometimes called data-dredging or p-hacking. To correct for this possibility, a trials factor can be applied to a statistical significance test. One method for coherently assessing p-values after multiple comparisons is the Bonferroni correction wherein the $\alpha$ significance required to reject the null hypothesis is modified by the number of trials, $n$, performed. This can be expressed, to first order, as (Include derivation? Full term is used in most recent LIGO IMBH serach):
\begin{equation}
    p\left(\mathrm{rejecting \, H_{0} \, due \, to \, any \, trial}| \mathrm{H_{0}} = \mathrm{True}\right) = \frac{\alpha}{n}.
\end{equation}
Implicit in the Bonferroni correction is that each new search for the parameter of interest is independent of the other. This provides the most conservative case for correcting against false positives, but it opens the possibility of increasing the rate of false negatives. In practice, multiple comparisons are not always statistically independent and so other significance level adjustments for multiple comparisons are possible.

\section{A Brief Introduction to Bayesian Statistics}
Another interpretation of probability and statistical inference is Bayesian inference, which relies more heavily on all of the aspects of Eq. \ref{eqn:BayesTheorem_basic}. In Bayesianism, inference is performed by stating our assumptions at the outset of our experiment. We must outright state the probability distributions that we take as given in order for our inference to be complete. Probabilities here reflect our implicit ignorance or our level of belief in a particular hypothesis. We will describe in some detail the steps of statistical inference in Bayesian statistics below.

\subsection{Bayesian Inference}
Before conducting the experiment we need to start at the outset and describe our prior beliefs about plausible measurements that we could arrive at from our experiment. In general, there is no ``correct" method for assigning prior probabilities to the range of plausible measurements, however we provide some helpful guidelines below. Let us recall that the prior probability distribution must obey the standard rules of probability as outlined in Eqs. \ref{eqn:probsumdiscrete}, \ref{eqn:probsumcontinuous}.

In order to better illustrate choices of prior distribution let us consider the example for the data generating process, Eq. \ref{eqn:Gaussian} from the previous section.

The first prior distribution that we suggest is the uniform prior distribution. A uniform prior in $x$ would suggest that we assign a uniform probability to all possible values of $x$ possible. The expression for the probability density of a uniform continuous distribution in some interval, $b < x < a$ is given as:
\begin{equation}
    \pi(x, H) = \frac{1}{b - a}
\end{equation}
Here we have substituted the notation $p(H)$ for the prior distribution with $\pi(x, H)$, the probability distribution function of $x$ given this particular hypothesis choice, $H$. In this particular case our data generating procedure extends over all real values of $x$. Formally extending $\pi$ to all values of $x$ would present a mathematical  problem. However, a reasonable choice of prior range can be chosen, such as $-10 < x < 10$, with, in this case, relatively little penalty.

The second prior distribution that we suggest is the informed prior distribution. Choosing an informed prior distribution is merely using our experimental expertise regarding the data generating procedure to provide a prior belief. Say for example, that the data generating procedure had been examined closely just prior to a recalibration of the machinery that generated the data. In a previous experiment, scientists estimated that the mean of a sample of the data was $0.12$, and with a variance of $1.02$. We might choose a prior distribution that used a Gaussian distribution with mean, $\mu = 0.12$, and variance, $\sigma^2 = 1.02$ for our inference. 

A third useful prior distribution to consider is a conjugate prior distribution. Conjugate prior distributions are sometimes available to certain kinds of likelihood distributions. If one has chosen a particular likelihood distribution and then the conjugate prior distribution to this likelihood distribution, then the posterior distribution will be in the same family of distributions as the conjugate prior distribution. This is advantageous if our prior beliefs on the family of the probability distribution that the data should belong to should not change with respect to the data. One example of a conjugate prior distribution is the conjugate prior distribution when choosing a Poisson likelihood distribution to measure the data. The conjugate prior distribution to the Poisson likelihood distribution is the Gamma distribution. The posterior distribution is then proportional to another Gamma distribution.

There are many other methods of selecting a prior distribution function, of which we will discuss in further chapters.

Finally, after a prior distribution is chosen, the likelihood can be measured from the data and inference on the probability distribution of $x$ can be estimated as the product of the prior and the likelihood. This product is sometimes called the joint probability distribution and it is proportional to the posterior probability distribution function. The posterior probability distribution reflects our updated beliefs about plausible values of $x$.

Finally, the term $P(D)$ is sometimes called the evidence or the prior predictive. We will use the short hand, $\mathcal{E}$ for evidence from now on. It is called the evidence because its value represents the level of credibility we should assign our inference relative to other prior hypothesis choices. In a similar vein, the term prior predictive gives us an idea of how well of a job we did in predicting the data with our choice of prior. Lastly, we can compute the evidence by computing the marginal likelihood. Formally, this is done via,
\begin{equation}\label{eqn:marg_likelihood}
    \mathcal{E} = \int \pi(\vec{\theta}) \mathcal{L}(\vec{\theta}) d\vec{\theta},
\end{equation}
where we have used $\pi(\vec{\theta})$ for the prior distribution over all parameters $\vec{\theta}$, and likelihood distribution $\mathcal{L}(\vec{\theta})$. The marginal likelihood, or evidence, will be useful in Bayesian hypothesis testing. Many times the marginal likelihood is a difficult multi-dimensional integral to compute. Although oftentimes we are only interested in the marginal posterior probability distribution function for parameters, leading us to achieve adequate statistical inference without computing Eq. \ref{eqn:marg_likelihood}. Computation of the marginal likelihood is not often required for parameter estimation.

\subsection{Parameter Estimation and Credible Intervals}
Bayesian inference covers a method for performing parameter estimation of a particular data set. In particular, we aim to extract meaningful inference on the parameters of our data from the posterior distribution. This is in contrast to Frequentist statistical inference which only relies on the likelihood distribution for statistical inference.

To do so, first we introduce the concept of marginalization of the posterior probability distribution function. Marginalization of a probability distribution is the process of finding a probability distribution of a given parameter, $A$, by integrating the joint distribution of $A$ with the other parameters, over all the values of the other parameters. This approach is taken in finding the marginal likelihood in Eq. \ref{eqn:marg_likelihood}, however it can be done for any parameter of interest. Marginalization for a continuous probability distribution can be expressed as,
\begin{equation}
    p(x) = \int p(x|\vec{\theta}\,') \, p(\vec{\theta}\,') \, d\vec{\theta}\,',
\end{equation}
where the integration occurs over $d\vec{\theta}\,'$, all variables in the parameter set excluding $x$. This procedure is also known as integrating out \textit{nuisance parameters}, or can be thought of as the expectation value of the probability distribution of $x$ after averaging over all other parameters. This procedure of marginalization provides a useful and consistent method for estimating plausible parameter values for a data set.

It is often useful to summarize the results of our inference through summary statistics such as the mean, median, standard deviation, or interquartile range. One such method is to design a credible interval based around a mean, median value, a maximum likelihood estimate (\textit{MLE}), or a maximum marginal posterior probability (in \textit{Latin} this is \textit{maximum a posteriori}, hence it is usually termed the \textit{MAP}) estimate.

A credible interval can be designed by determining some confidence level $\alpha$ wherein we desire that $\alpha$ percent of the posterior probability distribution is contained between some surface or interval. For a 1-dimensional marginalized probability distribution, $\mathcal{P}(x)$ this takes the form:
\begin{equation}\label{eqn:credible_interval}
    \alpha = \int^{x_{high}}_{x_{low}} \mathcal{P}(x) dx.
\end{equation}
Thus, one can say that the random variable $x$ is believed to have an $\alpha$ percent probability of being between the interval $x_{low}$ and $x_{high}$. This procedure of credible intervals is not required to be 1-dimensional, and can pertain to credible surface contours. Choosing where to set $x_{low}$ and $x_{high}$ is somewhat subjective, although choosing the $10^{th}$ and $90^{th}$ percentiles of the posterior probability is common practice. This is called an equal-tailed interval, which we will use for the remainder of this work. Other examples of credible interval include the highest posterior density interval and choosing an interval that centers around the mean parameter values or MAP values 1995.

\subsection{Bayesian Hypothesis Testing}
\subsubsection{The Bayes Factor}
Another essential aspect of Bayesian inference is the evaluation of the statistical signficance of hypothesis choices. This occurs through evaluating the effectiveness of the choice in prior probability distribution. The marginal likelihood, $\mathcal{E}$, is the main driver behind establishing the level of evidence or support that the data has for a particular prior distribution choice. Simply put, the prior distribution that results in the largest evidence value is the model that has the most support.

Calculation of the odds for support of one hypothesis, $H_1$, over another hypothesis, $H_2$, is encapsulated in the following expression for the posterior odds ratio:
\begin{equation}\label{eqn:odds_ratio}
\mathcal{O}^{H_1\;\;}_{\;\;H_2} = \mathcal{B}^{H_1\;\;}_{\;\;H_2} \times \frac{\pi(H_1)}{\pi(H_2)}.
\end{equation}
In this equation, $\mathcal{O}^{H_1\;\;}_{\;\;H_2}$ represents the posterior odds that hypothesis $1$ is preferred over hypothesis $2$. The ratio of the evidences, $\mathcal{B}^{H_1\;\;}_{\;\;H_2} \equiv \frac{\mathcal{E}_{H_1}}{\mathcal{E}_{H_2}}$, between the two models is known as the Bayes factor. The Bayes factor provides an intuition for the relative support of one hypothesis over the other. The ratio $\frac{\pi(H_1)}{\pi(H_2)}$ represents our prior odds ratio, that is, how much more did we believe that hypothesis $1$ was preferred over hypothesis $2$ prior to our analysis. Said in another way, the prior odds ratio gives us a statement of what level of Bayes factor we would require before we begin to change our minds about the odds of hypothesis $2$ being better supported in the data than hypothesis $1$. When testing new physics, one may set the prior odds ratio to unity if one is fundamentally unsure about what hypotheses the data may support.

The posterior odds ratio then gives us a method for making a decision about whether to accept one hypothesis over the other hypothesis. One advantage to Bayesian hypothesis testing is that it gives us a straightforward method for testing hypotheses other than the null hypothesis that is commonly tested in Frequentist statistical inference. The downside however is that effectively and consistently computing Bayes factors remains an open area of research because of how difficult it can be to calculate the marginal likelihood. A conventional choice for hypothesis decision making is given to us by Jeffreys, and an alternative by Kass and Raftery 1995, see Fig. X.

An odds ratio can be converted into a probability of one hypothesis over another hypothesis through the following expression:
\begin{equation}\label{eqn:probability_odds_ratio}
    p^{H_1 \;\;}_{\;\;H_2} = \frac{\mathcal{O}^{H_1\;\;}_{\;\;H_2}}{1 + \mathcal{O}^{H_1\;\;}_{\;\;H_2}}.
\end{equation}
As such, a plot of the $\mathrm{log}_{10} \; \mathcal{O}^{H_1\;\;}_{\;\;H_2}$ can be made to suggest decision rules for odds ratios similar to choices on p-values in Frequentist statistics. As we can see in the plot below, when the odds ratio is 1 ($\mathrm{log}_{10} \; \mathcal{O} = 0$) the probability of one hypothesis versus another is $0.5$. 
\begin{figure}
  \includegraphics[width=\linewidth]{figs/chapter2/log10odds_probability.png}
  \caption{The probability of hypothesis 1 being favored over hypothesis 2 when considering the $\mathrm{log}_{10} \; \mathcal{O}$. When $\mathrm{log}_{10} \; \mathcal{O} = 0$, the probability for each hypothesis is $50\%$. At odds ratios close to 100 (0.01) the evidence becomes heavily stacked towards one hypothesis or another.}
  \label{fig:log10odds_v_probability}
\end{figure}
Furthermore, we can map this probability to a ranking statistic that is more familiar to Frequentists. That is the one-tailed z-score which states the integrated probability density from $-\infty$ to a particular multiple of the standard deviation of a Gaussian function. A z-score of $0 \sigma$ indicates a $50\%$ probability, while a z-score of $5 \sigma$ is $\sim$ $1-10^{-7}$ probability. We place a plot of this below for convenience.
\begin{figure}
  \includegraphics[width=\linewidth]{figs/chapter2/log10odds_z_score.png}
  \caption{The Frequentist z-score pertaining to the same level of probability for  hypothesis 1 being favored over hypothesis 2 when considering the $\mathrm{log}_{10} \; \mathcal{O}$. When $\mathrm{log}_{10} \; \mathcal{O} = 0$, the z-score is $0 \sigma$ and the probability for each hypothesis is $50\%$. A z-score of $>5 \sigma$ has the same probability value as an odds ratio of $> 10^7$.}
  \label{fig:log10odds_v_z_score}
\end{figure}
