\section{Bayes factor approaches to Model Selection}
\subsection{Arithmetic Mean Estimator}
The simplest numerical integration estimation of the marginal likelihood makes use of a very large amount of samples drawn from the prior $\pi(\theta)$. For this estimator, Eq. N is approximated as:
\begin{equation}\label{AME}
    \mathcal{Z} \sim \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_i \left(\theta\right).
\end{equation}
Where N is the number of samples drawn from the prior. We can express Eq. \ref{AME} in the logarithmic form as:
\begin{equation}\label{logAME}
    ln \, \mathcal{Z} \sim  ln \, \mathcal{L}_\mathrm{max} + ln \, 
    \left [ \frac{1}{N} \sum_{i=1}^{N} e^{\left(ln \, \mathcal{L}_i - ln \, \mathcal{L}_\mathrm{max} \right)} \right ].
\end{equation}
While simple to compute, most practical applications have very sharply peaked posterior distributions relative to the prior distribution and so the arithmetic mean estimator can suffer from large variance (). Due to the disparity in the volume of the prior distribution and the posterior distribution, the arithmetic mean estimator can systematically underestimate of the marginal likelihood. In practice, this method requires an extremely large number of samples from the prior (even for low-dimensional models, typically $> 10^9 samples$).

Show average ln Z vs samples, with bootstrap estimated variance of ln Z vs samples. Very large. Not practical.

\subsection{Harmonic Mean Estimator}
The harmonic mean estimator tackles the problem of sharply peaked posterior distribution directly by averaging over the sum of the log-likelihood as drawn from the posterior distribution. Consider the following expression of the reciprocal of the marginal likelihood:
\begin{equation}\label{HMEsub1}
\int \frac{\mathcal{P}(\theta \mid D)}{\mathcal{L}\left(D \mid \theta\right)} d\theta. 
\end{equation}
If one substitutes $\mathcal{P}(\theta \mid D) = (1/\mathcal{Z}) \, \mathcal{L}(D \mid \theta) \, \pi (\theta)$ into Eq. \ref{HMEsub1}, one recovers the following expression:
\begin{equation}\notag
\frac{\int \pi(\theta) d\theta}{\mathcal{Z}} = \int \frac{\mathcal{P}(\theta \mid D)}{\mathcal{L}\left(D \mid \theta\right)} d\theta 
\end{equation}
Now, the prior distribution is always normalized to integrate to $1$ and so the above expression becomes:
\begin{equation}\label{HME}
\frac{1}{\mathcal{Z}} = \int \frac{\mathcal{P}(\theta \mid D)}{\mathcal{L}\left(D \mid \theta\right)} d\theta  = \left \langle \frac{1}{\mathcal{L}(D\mid \theta)} \right \rangle_{\mathcal{P}(\theta \mid D) }
\end{equation}
Thus the inverse of the marginal likelihood can be estimated by the expecation value of the inverse of the likelihood when drawn from the posterior distribution. It is sometimes stated in the statistics literature as, the expectation value of the inverse of the likelihood where the probability measure value is defined by the posterior distribution (). To estimate the marginal likelihood from actual discrete samples drawn from the posterior distribution we can rewrite Eq. \ref{HME} as:
\begin{equation}\label{estim_HME}
    \mathcal{Z} \sim \left(\frac{1}{N} \sum_{i=1}^{N} \frac{1}{\mathcal{L}_i} \right)^{-1}.
\end{equation}
Here $N$ represents the number of samples drawn from the posterior distribution. In the logarithmic form we can express this as:
\begin{equation}\label{estim_logHME}
    ln \, \mathcal{Z} = -ln \, \mathcal{L}_\mathrm{max} - ln \, \left [\frac{1}{N} \sum_{i=1}^{N} e^{-ln \, \mathcal{L}_i -ln \, \mathcal{L}_\mathrm{max}} \right].
\end{equation}
While drawing from the posterior may seem like a more efficient method of estimating the marginal likelihood than drawing from the prior this method is known to sometimes  overestimate the marginal likelihood. Furthermore, this estimator formally has an infinite variance, although the measured variance from samples may not be as large depending on the shape of the posterior distribution. Because of the large variance, the convergence to the marginal likelihood is exceedingly slow with an increase in the number of samples. Moreover, the harmonic mean estimator is numerically unstable and can often attempt to sum likelihoods approaching $1/0$. There have been several proposed improvements to the harmonic mean estimator lately, but we only consider this version of the estimator ().

Show explosion of HME to inf for simplest models, show how truncating posterior samples helps but is arbitrary for
unknown solution of marginal likelihood.

\subsection{The Laplace Approximation}
Useful to consider to introduce topic of Occam factor and why people are worried about the prior being too big, see LIGO model comparison
paper. Also generally wrong, making the prior bigger doesn't always make the Bayes factor shrink, it depends on the weighted average
of the likelihood across the prior. If a wider prior (e.g with new parameters) can do a better job of characterizing or predicting
the data it will have a much higher Bayes factor than a narrow prior.

\subsection{The Savage-Dickey Density Ratio for Nested Models}
The Savage-Dickey Density Ratio is a method for estimating Bayes factors directly rather than evaluating the marginal likelihood. The Bayes factor can be written as:
\begin{equation}
    \mathcal{B}^i_j = \frac{\mathcal{Z}_i}{\mathcal{Z}_j} = \frac{\int \pi \left(\theta_i \right) \, \mathcal{L}\left(\theta_i\right) d\theta_i}{\int \pi \left(\theta_j \right) \, \mathcal{L}\left(\theta_j\right) d\theta_j}
\end{equation}

Put in derivation from slides.

\subsection{Thermodynamic Integration Method}
UPDATE FROM latest Overleaf document

The thermodynamic integration method is a method for marginal likelihood evaluations that has its roots in statistical mechanics and was developed independently by Ogata and Gelman $\&$ Meng. The aim of the method is to convert the multidimensional integral of Eq. N into a one dimensional integral via a re-parametrization. Here we consider the power-posterior approach of Nial et al., etc. to thermodynamic integration. Consider the following expression for a power-posterior, $\mathcal{P}_\beta (\theta)$:
\begin{equation}\label{powerposterior}
    \mathcal{P}_\beta \left(\theta \right) \propto \pi\left(\theta \right) \, \mathcal{L}^\beta \left(\theta \right),
\end{equation}
where $\beta$ is an inverse-temperature, such that $0 \leq \beta \leq 1$. When $\beta = 1$, Eq. \ref{powerposterior} is the unmodified posterior distribution of Eq. N, and when $\beta = 0$, Eq. \ref{powerposterior} is equivalent to the prior distribution of Eq. N. Under this new parametrization we can now write the marginal likelihood for a particular $\beta$ as:
\begin{equation}\label{powerevidence}
    \mathcal{Z}_\beta = \int \pi \left(\theta\right) \, \mathcal{L}^\beta \left(\theta \right) \, d\theta.
\end{equation}
Under this parametrization we can consider the following expression from the 2nd fundamental theorem of Calculus:
\begin{equation}\label{TIidentity}
    ln \, \mathcal{Z}_{\beta=1} - ln \, \mathcal{Z}_{\beta=0} = \int^1_0 \frac{d\left(ln \, \mathcal{Z}_\beta \right)}{d\beta} \, d\beta.
\end{equation}
For a properly normalized prior, $\pi\left(\theta \right)$, $ln \, \mathcal{Z}_{\beta=0} = 0$. This leaves the marginal likelihood at $\beta=1$ that we are interested in, $ln \, \mathcal{Z}$. Combining Eq. \ref{TIidentity} with Eq. \ref{powerevidence} we arrive at an expression that can be numerically calculated via MCMC simulation of power-posterior distributions:
\begin{equation}\label{TIeq}
    ln \, \mathcal{Z} = \int^1_0 \langle ln \, \mathcal{L} \rangle_\beta \, d\beta.
\end{equation}
Thus, the marginal likelihood can be estimated by integrating the expectation value of the log-likelihood $ln \, \mathcal{L}$ of a power-posterior distribution at each inverse-temperature $\beta$ over the inverse-temperatures between $0$ and $1$.

\subsubsection{Numerical Integration Techniques}\label{subsubsec:NumTI}

Numerically estimating the marginal likelihood via thermodynamic integration can be done by selecting a finite set of inverse-temperatures $\beta$ to simulate the power-posterior distributions via ensemble MCMC methods. In Eq. \ref{TIidentity}, we require an estimation of the expectation value of the log-likelihood for every power-posterior. This can be done via the following expression:
\begin{equation}\label{expectedlogl}
    \langle ln \, \mathcal{L} \rangle_\beta \equiv \frac{1}{N}\sum^N_{i=1} ln \, \mathcal{L}_i,
\end{equation}
Here, the expectation value of the log-likelihood of a particular power-posterior is estimated via finding the mean log-likelihood for that power-posterior. In Eq.\ref{expectedlogl}, $N$ represents the number of (independent) samples in the power-posterior distribution.

Following this, we can begin to numerically calculate the marginal likelihood from a set of inverse-temperatures $\beta$. For the rest of the section we will use the following inverse-temperature convention: $\beta_1=0 < \beta_2 < ... < \beta_{N_\beta -1} < \beta_{N_\beta} = 1$. Here, $N_\beta$ represents the Nth inverse-temperature chosen. The simplest numerical integration approximation of Eq. \ref{TIeq} is to use the method of Riemann sums,
\begin{equation}\label{TIRiemann}
    ln \, \mathcal{Z} \sim \sum^{N_{\beta-1}}_{i=1} \left( \beta_{i+1} - \beta_{i} \right ) \langle ln \, \mathcal{L} \rangle_{\beta_i}.
\end{equation}
While straightforward, there are higher order quadrature rules that can be used to give a more faithful representation of the marginal likelihood. We can go to the next order polynomial integration method and use the composite trapezoidal rule:
\begin{equation}\label{TITrapz}
    ln \, \mathcal{Z} \sim \sum^{N_{\beta-1}}_{i=1} \frac{\left( \beta_{i+1} - \beta_{i} \right )}{2}  \left(\langle ln \, \mathcal{L} \rangle_{\beta_{i+1}} + \langle ln \, \mathcal{L} \rangle_{\beta_i} \right).
\end{equation}

STREAMLINE BELOW

Other works have considered similarly using Simpson's rule of numerical integration with parabolas between 3 points. Generally speaking, Simpson's rule requires an even number of points to integrate over. This is only a mild drawback to the method in that, for an odd number of temperatures, one can integrate over all but one last $\beta$, and then use the trapezoid rule to estimate the integral over the last $\beta$. Standard formulations of Simpson's rule usually require an equal spacing of $\beta$ or a change of variable from some power-law distribution of $\beta$ to an equally spaced set of points to integrate over. For an equal-spacing choice of $\beta$, which we denote as $\Delta \beta$, an estimate of Eq. \ref{TIeq} with Simpson's rule can be expressed as:
\begin{equation}\label{TIequalSimpsons}
\begin{split}
    ln \, \mathcal{Z} \sim \frac{\Delta \beta}{3} \sum^{(N_{\beta}-2)/2}_{i=1} &
    \big(\langle ln \, \mathcal{L} \rangle_{\beta_{2i}} + 4 \langle ln \, \mathcal{L} \rangle_{\beta_{2i+1}} \\
    & \quad + \langle ln \, \mathcal{L} \rangle_{\beta_{2i+2}} \big).
\end{split}
\end{equation}

REPLACE THIS
As will later be discussed, choosing a set of inverse-temperatures that are equidistant will be sub-optimal and so we introduce a formulation of Simpson's rule that does not depend on equally spaced data points. Thus, Simpson's Rule for unequally spaced data can be used to estimate Eq. \ref{TIeq} via the following expression:
\begin{equation}\label{TIunequalSimpsons}
\begin{split}
ln \, \mathcal{Z} \sim \sum_{i=1}^{(N_\beta-2) / 2} & \frac{h_{2i} + h_{2i+1}}{6}
 \bigg [\left(\frac{2h_{2i} - h_{2i+1}}{h_{2i}} \right) \langle ln \, \mathcal{L} \rangle_{\beta_{2i}} \\
& \quad  + \left(\frac{(h_{2i} + h_{2i+1})^2}{h_{2i} \, h_{2i+1}} \right) \langle ln \, \mathcal{L} \rangle_{\beta_{2i+1}}  \\
& \quad + \left(\frac{2h_{2i+1} - h_{2i}}{h_{2i+1}} \right) \langle ln \, \mathcal{L} \rangle_{\beta_{2i+2}} \bigg ].
\end{split}
\end{equation}
Here we have used $h_{2i} \equiv \beta_{2i+1} - \beta_{2i}$. As in Eq. \ref{TIequalSimpsons}, Eq. \ref{TIunequalSimpsons} requires an even number of $\beta$ points. In the case that one has an odd number of points, one can always use the trapezoidal rule on the last $\beta$ value to complete the numerical integration. Generally, the log of the marginal likelihood tends to get very small contributions from $\beta$ near 0, and so in the event of an odd number of temperatures we recommend performing the trapezoidal rule for this final $\beta$ near $\beta = 0$.

We can continue this program of increasing the order of quadrature for Eq.~\ref{TIeq}, however higher order quadrature numerical integration generally leads to considerations of Gaussian quadrature integration techniques. Gaussian quadrature integration typically requires being able to easily evaluate values of the integrand at different points, however, it is generally expensive to generate a new $\langle ln \, \mathcal{L}\rangle_\beta$, and so this motivates  generating an interpolating function over all $\langle ln \, \mathcal{L}\rangle_\beta$. One possible method for doing this is to use a piecewise polynomial or cubic spline interpolant between $\beta$. We will explore the possibility of interpolation in the following section.

\subsubsection{Improving numerical integration by including derivative information}\label{subsubsec:TIDerivs}
FIX with UPDATED OVERLEAF

The nth derivatives of $ln \, \mathcal{Z}$ can be found according to the following expression:
\begin{equation}\label{TIderivatives}
    \frac{d^n \left(ln \, \mathcal{Z} \right)}{d\beta^n} = \sum^n_{k=1} \frac{\left(-1 \right)^{k+1} \binom{n}{k}}{k \, \mathcal{Z}^k} \frac{d^n}{d\beta^n} \left( \mathcal{Z}^k\right)
\end{equation}
Thus for $n=1$, we get $\frac{d \, \left(ln \,\mathcal{Z}\right)}{d \beta}$ = $\frac{\mathcal{Z}'}{\mathcal{Z}}$ = $\langle ln \mathcal{L} \rangle_\beta$, as expected. For $n=2$, we get, $-\frac{\mathcal{Z}'^2}{\mathcal{Z}^2} + \frac{\mathcal{Z}''}{\mathcal{Z}}$, which further reduces to $-\langle ln \, \mathcal{L}\rangle^2_\beta + \langle \left(ln \, \mathcal{L} \right)^2\rangle_\beta$. This expression is the variance of the log-likelihood at a particular power-posterior, $\mathrm{Var} \left(ln \, \mathcal{L} \right)_\beta$. For those familiar with statistical mechanics in physics, the higher order derivatives of the log of the partition function $\mathcal{Z}$ also represent higher order cumulants of that partition function (citation needed). The same is true here of the thermodynamic integrand.

In order to give the leading order error estimate for the trapezoidal rule, we re-express the first derivative of the thermodynamic integrand, using Eq. \ref{TIderivatives} as the following:
\begin{equation}\label{TrapzDeriv}
    \zeta_\beta = \mathrm{Var}\left(ln \, \mathcal{L}\right)_\beta.
\end{equation}
Then we can express the leading order correction to the trapezoidal rule as follows:
\begin{equation}\label{TrapzError}
    \mathrm{Err(Trapz)} = \left \{
              \begin{array}{ll}
              - \frac{\left(x_{i+1} - x_i\right)^2}{12} \left(f'(x_{i+1}) - f'(x_i)\right)   \\ \\
              -\frac{\left(\beta_{i+1} - \beta_i\right)^2}{12} \left(\zeta_{\beta_{i+1}}
               - \zeta_{\beta_{i}} \right)
              \end{array}
              \right.
\end{equation}
And for Simpson's rule we can also use Eq. \ref{TIderivatives} to get the third derivative of the thermodynamic integrand (this is the fourth derivative of $ln \, \mathcal{Z}$) as:
\begin{equation}\label{SimpsDeriv}
\begin{split}
    \xi_\beta & \equiv 6 \langle ln \, \mathcal{L} \rangle^4_\beta + 12 \langle ln \, \mathcal{L} \rangle^2_\beta \langle \left(ln \, \mathcal{L}\right)^2\rangle_\beta  \\
    &\quad - 3 \langle \left(ln \, \mathcal{L} \right)^2\rangle^2_\beta - 4 \langle ln \, \mathcal{L} \rangle_\beta \langle \left( ln \, \mathcal{L}\right)^3\rangle_\beta \\
    &\quad + \langle \left(ln \, \mathcal{L}\right)^4\rangle_\beta 
\end{split}
\end{equation}
Thus, using Eq. Simpson's rule to leading order correction is given as:
NOPE ADD IN FROM RECENT OVERLEAF

A word of caution is necessary for calculating higher order derivatives / cumulants of $ln \, \mathcal{Z}$ on the computer. The log-likelihood tends to be a very large number in most practical applications, and so floating-point arithmetic will tend to fail very rapidly unless additional computational techniques are applied. Fortunately, one property of cumulants of order $n \ge 2$ (i.e. the variance and higher order) is that they are invariant to shifts in the data set. For this reason we propose that when calcualting higher order derivatives / cumulants one prepare the data set according to the following transformation:
\begin{equation}\label{logltransform}
    \widehat{ln \, \mathcal{L}} \equiv ln \, \mathcal{L} - ln \, \mathcal{L}_\mathrm{max}.
\end{equation}
In our data sets we typically have $ln \, \mathcal{L}$ of order $10^7$, and so this transformation is necessary for numerical fidelity of the calculation of higher order derivatives.


\subsection{The Steppingstone Algorithm}

The steppingstone method is very similar in many respects to thermodynamic integration in that it requires multiple inverse-temperatures between $0$ and $1$ to calculate. The motivation for steppingstone is that it uses importance sampling between adjacent temperatures to estimate the contribution to the marginal likelihood $\mathcal{Z}$. Recall from Eq. \ref{TIidentity} that the marginal likelihood can be expressed as:
\begin{equation}\notag
    ln \, \mathcal{Z} = ln \, \mathcal{Z}_{\beta=1} - ln \, \mathcal{Z}_{\beta=0},
\end{equation}
which is equivalent to:
\begin{equation}\notag
    \mathcal{Z} = \frac{\mathcal{Z}_{\beta=1}}{\mathcal{Z}_{\beta=0}}.
\end{equation}
For, say, a hundred equally spaced temperatures between $0$ and $1$ this motivates the following re-expression:
\begin{equation}\notag
    \mathcal{Z} = \frac{\mathcal{Z}_{\beta = 0.01}} {\mathcal{Z}_{\beta = 0}}  
    \times \frac{\mathcal{Z}_{\beta = 0.02}} {\mathcal{Z}_{\beta = 0.01}}
    \times \ldots \times \frac{\mathcal{Z}_{\beta = 0.99}} {\mathcal{Z}_{\beta = 0.98}} \times \frac{\mathcal{Z}_{\beta = 1}} {\mathcal{Z}_{\beta = 0.99}}.
\end{equation}
The general form for this is:
\begin{equation}\label{SSsub1}
     \mathcal{Z} = \prod_{i=1}^{N_\beta -1} \frac{\mathcal{Z}_{\beta_{i+1}}}{\mathcal{Z}_{\beta_{i}}}.
\end{equation}
Here we use the same ordering as in Section \ref{subsec:TI}, i.e. $\beta_1=0 < \beta_2 < ... < \beta_{N_\beta -1} < \beta_{N_\beta} = 1$. We can re-express Eq. \ref{SSsub1} as the following then:
\begin{equation}\notag
     \mathcal{Z} = \prod_{i=1}^{N_\beta -1} \int p(\theta) \frac{ \mathcal{L}^{\beta_{i+1}}}{\mathcal{L}^{\beta_{i}}} \, d\theta.
\end{equation}
Finally, then we can construct the steppingstone estimator:
\begin{equation}\label{SSeq1}
    \mathcal{Z} \sim \prod_{i=1}^{N_\beta -1} \frac{1}{N} \sum_j^N \mathcal{L}_j^{\beta_{i+1} - \beta_{i}} = \prod_{i=1}^{N_\beta -1} \langle \mathcal{L}^{\left(\beta_{i+1} - \beta_{i}\right)}\rangle_{\beta_i}.
\end{equation}
Note that while in thermodynamic integration, a la Eq. \ref{TIeq}, we deal with the average log-likelihood, in the steppingstone estimator we work with the average likelihood instead. Furthermore, if there is only one inverse-temperature, Eq. \ref{SSeq1} reduces to the arithmetic mean estimator. Furthermore, we can express Eq. \ref{SSeq1} in a logarithmic form:
\begin{equation}\label{SSeq2}
    ln \, \mathcal{Z} \sim \sum_{i=1}^{N_\beta -1} ln \, \langle \mathcal{L}^{\left(\beta_{i+1} - \beta_{i}\right)}\rangle_{\beta_i}.
\end{equation}
Xie. et al provide a numerically stable method for estimating Eq.~\ref{sseq2}. It is noted that while Eq. \ref{SSeq1} is a generally unbiased estimator the marginal likelihood, Eq. \ref{SSeq2} introduces some level of bias to the estimator. This bias is minimized by an increased number of temperatures. Xie et al also provide a useful estimate for the variance of Eq. \ref{SSeq2}.

For the steppingstone estimator we can always use the same samples as for the thermodynamic integration method, although, more research may need to be done on optimizing the temperature ladder for the steppingstone estimator.
\subsection{Nested Sampling}
Laplace transform of marginal likelihood equation, similar to TI, but different approach, density of states approach, etc.

Doesn't require as much fine tuning as TI, also these samplers tend to be optimized for evidence whereas EmceePT and PTEmcee are not optimized for evidence solving. Talk about Dynesty

Keep it brief.

\section{Information Theoretic approaches}
Calculating marginal likelihoods and Bayes factors are not the only method for doing model comparison. Other approaches include using information theoretic approaches towards evaluating model-fitness and predictiveness.

AIC, BIC, WAIC, DIC

Compare with loss-functions from recent Gabbard paper (https://arxiv.org/pdf/1909.06296.pdf)

Keep it brief.
